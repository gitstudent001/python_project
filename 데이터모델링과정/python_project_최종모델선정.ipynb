{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296e1658-8f17-4126-828c-0d30f053e041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import wordcloud\n",
    "from wordcloud import WordCloud\n",
    "import json\n",
    "import folium\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "# 한글 설정\n",
    "# pip install koreanize_matplotlib\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "root = 'C:/workspace/python/project/data/'\n",
    "\n",
    "# 전처리 완료한 데이터파일 이동경로\n",
    "pre_root = 'C:/workspace/python/project/data/_전처리/'\n",
    "\n",
    "# 구글드라이브 : https://drive.google.com/drive/folders/1zIzm1o8-3uxcWSU2DoWpB8aV0Oxdfz_P?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456056dd-5c73-4677-92d7-3d7fb89f5843",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "                         /////--     데이터 불러오기 (희만)     --/////\n",
    "'''\n",
    "# 공원\n",
    "park_2015_df = pd.read_csv(pre_root + '공원_Data/전처리_공원수_2015.csv', encoding = 'cp949')\n",
    "park_2016_df = pd.read_csv(pre_root + '공원_Data/전처리_공원수_2016.csv', encoding = 'cp949')\n",
    "park_2017_df = pd.read_csv(pre_root + '공원_Data/전처리_공원수_2017.csv', encoding = 'cp949')\n",
    "park_2018_df = pd.read_csv(pre_root + '공원_Data/전처리_공원수_2018.csv', encoding = 'cp949')\n",
    "park_2019_df = pd.read_csv(pre_root + '공원_Data/전처리_공원수_2019.csv', encoding = 'cp949')\n",
    "park_2020_df = pd.read_csv(pre_root + '공원_Data/전처리_공원수_2020.csv', encoding = 'cp949')\n",
    "\n",
    "park_2021_df = pd.read_csv(pre_root + '공원_Data/전처리_공원수_2021.csv', encoding = 'cp949')\n",
    "park_2022_df = pd.read_csv(pre_root + '공원_Data/전처리_공원수_2022.csv', encoding = 'cp949')\n",
    "park_2023_df = pd.read_csv(pre_root + '공원_Data/전처리_공원수_2023.csv', encoding = 'cp949')\n",
    "\n",
    "# 교통\n",
    "## 버스\n",
    "bus_2019_df = pd.read_csv(pre_root + '교통_Data/전처리_버스수_2019.csv', encoding = 'cp949')\n",
    "bus_2020_df = pd.read_csv(pre_root + '교통_Data/전처리_버스수_2020.csv', encoding = 'cp949')\n",
    "\n",
    "bus_2021_df = pd.read_csv(pre_root + '교통_Data/전처리_버스수_2021.csv', encoding = 'cp949')\n",
    "bus_2022_df = pd.read_csv(pre_root + '교통_Data/전처리_버스수_2022.csv', encoding = 'cp949')\n",
    "bus_2023_df = pd.read_csv(pre_root + '교통_Data/전처리_버스수_2023.csv', encoding = 'cp949')\n",
    "## 지하철\n",
    "train_2015_df = pd.read_csv(pre_root + '교통_Data/전처리_지하철수_2015.csv', encoding = 'cp949')\n",
    "train_2020_df = pd.read_csv(pre_root + '교통_Data/전처리_지하철수_2020.csv', encoding = 'cp949')\n",
    "\n",
    "train_2021_df = pd.read_csv(pre_root + '교통_Data/전처리_지하철수_2021.csv', encoding = 'cp949')\n",
    "train_2022_df = pd.read_csv(pre_root + '교통_Data/전처리_지하철수_2022.csv', encoding = 'cp949')\n",
    "train_2023_df = pd.read_csv(pre_root + '교통_Data/전처리_지하철수_2023.csv', encoding = 'cp949')\n",
    "\n",
    "# 부동산\n",
    "## 서울시 집값\n",
    "house_2015_df = pd.read_csv(pre_root + '부동산_Data/전처리_부동산_2015.csv', encoding = 'cp949')\n",
    "house_2016_df = pd.read_csv(pre_root + '부동산_Data/전처리_부동산_2016.csv', encoding = 'cp949')\n",
    "house_2017_df = pd.read_csv(pre_root + '부동산_Data/전처리_부동산_2017.csv', encoding = 'cp949')\n",
    "house_2018_df = pd.read_csv(pre_root + '부동산_Data/전처리_부동산_2018.csv', encoding = 'cp949')\n",
    "house_2019_df = pd.read_csv(pre_root + '부동산_Data/전처리_부동산_2019.csv', encoding = 'cp949')\n",
    "house_2020_df = pd.read_csv(pre_root + '부동산_Data/전처리_부동산_2020.csv', encoding = 'cp949')\n",
    "\n",
    "house_2021_df = pd.read_csv(pre_root + '부동산_Data/전처리_부동산_2021.csv', encoding = 'cp949')\n",
    "house_2022_df = pd.read_csv(pre_root + '부동산_Data/전처리_부동산_2022.csv', encoding = 'cp949')\n",
    "house_2023_df = pd.read_csv(pre_root + '부동산_Data/전처리_부동산_2023.csv', encoding = 'cp949')\n",
    "## 개발계획\n",
    "develop_2015_df = pd.read_csv(pre_root + '부동산_Data/전처리_개발계획_2015.csv', encoding = 'cp949')\n",
    "develop_2016_df = pd.read_csv(pre_root + '부동산_Data/전처리_개발계획_2016.csv', encoding = 'cp949')\n",
    "develop_2017_df = pd.read_csv(pre_root + '부동산_Data/전처리_개발계획_2017.csv', encoding = 'cp949')\n",
    "develop_2018_df = pd.read_csv(pre_root + '부동산_Data/전처리_개발계획_2018.csv', encoding = 'cp949')\n",
    "develop_2019_df = pd.read_csv(pre_root + '부동산_Data/전처리_개발계획_2019.csv', encoding = 'cp949')\n",
    "develop_2020_df = pd.read_csv(pre_root + '부동산_Data/전처리_개발계획_2020.csv', encoding = 'cp949')\n",
    "\n",
    "develop_2021_df = pd.read_csv(pre_root + '부동산_Data/전처리_개발계획_2021.csv', encoding = 'cp949')\n",
    "develop_2022_df = pd.read_csv(pre_root + '부동산_Data/전처리_개발계획_2022.csv', encoding = 'cp949')\n",
    "develop_2023_df = pd.read_csv(pre_root + '부동산_Data/전처리_개발계획_2023.csv', encoding = 'cp949')\n",
    "\n",
    "# 수요공급\n",
    "demandSupply_2015_df = pd.read_csv(pre_root + '수요공급_Data/전처리_수요공급지수_2015.csv', encoding = 'cp949')\n",
    "demandSupply_2016_df = pd.read_csv(pre_root + '수요공급_Data/전처리_수요공급지수_2016.csv', encoding = 'cp949')\n",
    "demandSupply_2017_df = pd.read_csv(pre_root + '수요공급_Data/전처리_수요공급지수_2017.csv', encoding = 'cp949')\n",
    "demandSupply_2018_df = pd.read_csv(pre_root + '수요공급_Data/전처리_수요공급지수_2018.csv', encoding = 'cp949')\n",
    "demandSupply_2019_df = pd.read_csv(pre_root + '수요공급_Data/전처리_수요공급지수_2019.csv', encoding = 'cp949')\n",
    "demandSupply_2020_df = pd.read_csv(pre_root + '수요공급_Data/전처리_수요공급지수_2020.csv', encoding = 'cp949')\n",
    "\n",
    "demandSupply_2021_df = pd.read_csv(pre_root + '수요공급_Data/전처리_수요공급지수_2021.csv', encoding = 'cp949')\n",
    "demandSupply_2022_df = pd.read_csv(pre_root + '수요공급_Data/전처리_수요공급지수_2022.csv', encoding = 'cp949')\n",
    "demandSupply_2023_df = pd.read_csv(pre_root + '수요공급_Data/전처리_수요공급지수_2023.csv', encoding = 'cp949')\n",
    "\n",
    "# 유통업체\n",
    "distribute_2015_df = pd.read_csv(pre_root + '유통업체_Data/전처리_유통업체_2015.csv', encoding = 'cp949')\n",
    "distribute_2016_df = pd.read_csv(pre_root + '유통업체_Data/전처리_유통업체_2016.csv', encoding = 'cp949')\n",
    "distribute_2017_df = pd.read_csv(pre_root + '유통업체_Data/전처리_유통업체_2017.csv', encoding = 'cp949')\n",
    "distribute_2018_df = pd.read_csv(pre_root + '유통업체_Data/전처리_유통업체_2018.csv', encoding = 'cp949')\n",
    "distribute_2019_df = pd.read_csv(pre_root + '유통업체_Data/전처리_유통업체_2019.csv', encoding = 'cp949')\n",
    "distribute_2020_df = pd.read_csv(pre_root + '유통업체_Data/전처리_유통업체_2020.csv', encoding = 'cp949')\n",
    "\n",
    "distribute_2021_df = pd.read_csv(pre_root + '유통업체_Data/전처리_유통업체_2021.csv', encoding = 'cp949')\n",
    "distribute_2022_df = pd.read_csv(pre_root + '유통업체_Data/전처리_유통업체_2022.csv', encoding = 'cp949')\n",
    "distribute_2023_df = pd.read_csv(pre_root + '유통업체_Data/전처리_유통업체_2023.csv', encoding = 'cp949')\n",
    "\n",
    "# 의료기관\n",
    "hospital_2015_df = pd.read_csv(pre_root + '의료기관_Data/전처리_병원수_2015.csv', encoding = 'cp949')\n",
    "hospital_2016_df = pd.read_csv(pre_root + '의료기관_Data/전처리_병원수_2016.csv', encoding = 'cp949')\n",
    "hospital_2017_df = pd.read_csv(pre_root + '의료기관_Data/전처리_병원수_2017.csv', encoding = 'cp949')\n",
    "hospital_2018_df = pd.read_csv(pre_root + '의료기관_Data/전처리_병원수_2018.csv', encoding = 'cp949')\n",
    "hospital_2019_df = pd.read_csv(pre_root + '의료기관_Data/전처리_병원수_2019.csv', encoding = 'cp949')\n",
    "hospital_2020_df = pd.read_csv(pre_root + '의료기관_Data/전처리_병원수_2020.csv', encoding = 'cp949')\n",
    "\n",
    "hospital_2021_df = pd.read_csv(pre_root + '의료기관_Data/전처리_병원수_2021.csv', encoding = 'cp949')\n",
    "hospital_2022_df = pd.read_csv(pre_root + '의료기관_Data/전처리_병원수_2022.csv', encoding = 'cp949')\n",
    "hospital_2023_df = pd.read_csv(pre_root + '의료기관_Data/전처리_병원수_2023.csv', encoding = 'cp949')\n",
    "\n",
    "# 인구수\n",
    "population_2015_df = pd.read_csv(pre_root + '인구수_Data/전처리_인구수_2015.csv', encoding = 'cp949')\n",
    "population_2016_df = pd.read_csv(pre_root + '인구수_Data/전처리_인구수_2016.csv', encoding = 'cp949')\n",
    "population_2017_df = pd.read_csv(pre_root + '인구수_Data/전처리_인구수_2017.csv', encoding = 'cp949')\n",
    "population_2018_df = pd.read_csv(pre_root + '인구수_Data/전처리_인구수_2018.csv', encoding = 'cp949')\n",
    "population_2019_df = pd.read_csv(pre_root + '인구수_Data/전처리_인구수_2019.csv', encoding = 'cp949')\n",
    "population_2020_df = pd.read_csv(pre_root + '인구수_Data/전처리_인구수_2020.csv', encoding = 'cp949')\n",
    "\n",
    "population_2021_df = pd.read_csv(pre_root + '인구수_Data/전처리_인구수_2021.csv', encoding = 'cp949')\n",
    "population_2022_df = pd.read_csv(pre_root + '인구수_Data/전처리_인구수_2022.csv', encoding = 'cp949')\n",
    "population_2023_df = pd.read_csv(pre_root + '인구수_Data/전처리_인구수_2023.csv', encoding = 'cp949')\n",
    "\n",
    "# 주거실태 (거래량)\n",
    "volume_2015_df = pd.read_csv(pre_root + '주거실태_Data/전처리_거래량_2015.csv', encoding = 'cp949')\n",
    "volume_2016_df = pd.read_csv(pre_root + '주거실태_Data/전처리_거래량_2016.csv', encoding = 'cp949')\n",
    "volume_2017_df = pd.read_csv(pre_root + '주거실태_Data/전처리_거래량_2017.csv', encoding = 'cp949')\n",
    "volume_2018_df = pd.read_csv(pre_root + '주거실태_Data/전처리_거래량_2018.csv', encoding = 'cp949')\n",
    "volume_2019_df = pd.read_csv(pre_root + '주거실태_Data/전처리_거래량_2019.csv', encoding = 'cp949')\n",
    "volume_2020_df = pd.read_csv(pre_root + '주거실태_Data/전처리_거래량_2020.csv', encoding = 'cp949')\n",
    "\n",
    "volume_2021_df = pd.read_csv(pre_root + '주거실태_Data/전처리_거래량_2021.csv', encoding = 'cp949')\n",
    "volume_2022_df = pd.read_csv(pre_root + '주거실태_Data/전처리_거래량_2022.csv', encoding = 'cp949')\n",
    "volume_2023_df = pd.read_csv(pre_root + '주거실태_Data/전처리_거래량_2023.csv', encoding = 'cp949')\n",
    "\n",
    "# 주거실태\n",
    "abode_house_2015_df = pd.read_csv(pre_root + '주거실태_Data/전처리_주거실태_2015.csv', encoding = 'cp949')\n",
    "abode_house_2016_df = pd.read_csv(pre_root + '주거실태_Data/전처리_주거실태_2016.csv', encoding = 'cp949')\n",
    "abode_house_2017_df = pd.read_csv(pre_root + '주거실태_Data/전처리_주거실태_2017.csv', encoding = 'cp949')\n",
    "abode_house_2018_df = pd.read_csv(pre_root + '주거실태_Data/전처리_주거실태_2018.csv', encoding = 'cp949')\n",
    "abode_house_2019_df = pd.read_csv(pre_root + '주거실태_Data/전처리_주거실태_2019.csv', encoding = 'cp949')\n",
    "abode_house_2020_df = pd.read_csv(pre_root + '주거실태_Data/전처리_주거실태_2020.csv', encoding = 'cp949')\n",
    "\n",
    "abode_house_2021_df = pd.read_csv(pre_root + '주거실태_Data/전처리_주거실태_2021.csv', encoding = 'cp949')\n",
    "abode_house_2022_df = pd.read_csv(pre_root + '주거실태_Data/전처리_주거실태_2022.csv', encoding = 'cp949')\n",
    "abode_house_2023_df = pd.read_csv(pre_root + '주거실태_Data/전처리_주거실태_2023.csv', encoding = 'cp949')\n",
    "\n",
    "# 매매가격지수\n",
    "priceRelative_2015_df = pd.read_csv(pre_root + '수요공급_Data/전처리_매매가격지수_2015.csv', encoding = 'cp949')\n",
    "priceRelative_2016_df = pd.read_csv(pre_root + '수요공급_Data/전처리_매매가격지수_2016.csv', encoding = 'cp949')\n",
    "priceRelative_2017_df = pd.read_csv(pre_root + '수요공급_Data/전처리_매매가격지수_2017.csv', encoding = 'cp949')\n",
    "priceRelative_2018_df = pd.read_csv(pre_root + '수요공급_Data/전처리_매매가격지수_2018.csv', encoding = 'cp949')\n",
    "priceRelative_2019_df = pd.read_csv(pre_root + '수요공급_Data/전처리_매매가격지수_2019.csv', encoding = 'cp949')\n",
    "priceRelative_2020_df = pd.read_csv(pre_root + '수요공급_Data/전처리_매매가격지수_2020.csv', encoding = 'cp949')\n",
    "\n",
    "priceRelative_2021_df = pd.read_csv(pre_root + '수요공급_Data/전처리_매매가격지수_2021.csv', encoding = 'cp949')\n",
    "priceRelative_2022_df = pd.read_csv(pre_root + '수요공급_Data/전처리_매매가격지수_2022.csv', encoding = 'cp949')\n",
    "priceRelative_2023_df = pd.read_csv(pre_root + '수요공급_Data/전처리_매매가격지수_2023.csv', encoding = 'cp949')\n",
    "\n",
    "# 서울시 면적\n",
    "seoul_area_df = pd.read_csv(pre_root + '서울시_자치구/전처리_면적_2023.csv', encoding = 'cp949')\n",
    "\n",
    "# 주택담보금리\n",
    "rate_df = pd.read_csv(pre_root + '부동산_Data/전처리_주택담보금리.csv', encoding = 'cp949')\n",
    "\n",
    "print('데이터 프레임 삽입 완료..!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51550a1e-0c2e-48bd-95e5-afc98a446e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "                         /////--     데이터 병합 함수 (희만)     --/////\n",
    "          공원, 버스, 지하철, 부동산, 개발계획, 수요공급, 유통업체, 의료기관, 인구수, 거래량\n",
    "'''\n",
    "def merge_year_dataFrame(park_df, bus_df, train_df, house_df, develop_df, demandSupply_df,\n",
    "                         distribute_df, hospital_df, population_df, volume_df,\n",
    "                         abode_house_df, priceRelative_df, seoul_area_df, rate_df, year) :\n",
    "  # 공원\n",
    "  park_df = park_df[['구', '합계_공원수 (개소)']].rename(columns = {'합계_공원수 (개소)' : '합계_공원수'})\n",
    "  \n",
    "  # 버스\n",
    "  if bus_df.drop(columns=['구'], axis=1).isnull().all().all() : \n",
    "    bus_df = pd.DataFrame({'구' : house_2021_df['구'], '버스 수' : np.nan})  \n",
    "  else : \n",
    "    bus_df = bus_df['구'].value_counts().reset_index().rename(columns = {'count' : '버스 수'})\n",
    "  \n",
    "  # 지하철\n",
    "  train_df = train_df[['구', '역개수']]\n",
    "  \n",
    "  # 부동산\n",
    "  house_df = house_df[['구', '평균시세']]\n",
    "  \n",
    "  # 개발계획\n",
    "  develop_df = develop_df[['구', '계_구역수 (개)', '계_면적 (㎡)']].rename(columns = {'계_구역수 (개)' : '개발계획_합계'})\n",
    "  \n",
    "  # 수요공급\n",
    "  demandSupply_df = round(demandSupply_df.groupby('구')['수급등급'].mean(), 2).reset_index().rename(columns = {'수급등급' : '수요공급지수'})\n",
    "  \n",
    "  # 유통업체\n",
    "  distribute_df = distribute_df.iloc[1:, :].reset_index(drop = True)\n",
    "  condition = distribute_df.columns.str.contains('_개소') # 원하는 컬럼만 조회\n",
    "  condition[0] = True\n",
    "  distribute_df = distribute_df.loc[:, condition]\n",
    "  column_names = {} # 컬럼 이름 변경용 딕셔너리\n",
    "  columns = distribute_df.columns\n",
    "  for column in columns :\n",
    "    column_names[column] = column.split('_개소')[0]\n",
    "  distribute_df = distribute_df.rename(columns = column_names)\n",
    "  distribute_df = distribute_df.replace('-', 0)\n",
    "  \n",
    "  # 의료기관\n",
    "  hospital_df = hospital_df[['구', '소계_병원수']].rename(columns = {'소계_병원수' : '병원수'})\n",
    "  hospital_df = hospital_df.iloc[1:, :].reset_index(drop = True)\n",
    "  \n",
    "  # 인구수\n",
    "  population_df\n",
    "  \n",
    "  # 주거실태 (거래량)\n",
    "  volume_df = volume_df.drop(columns = '년도').rename(columns = {'동(호)수' : '거래량'})[['구', '거래량']]\n",
    "\n",
    "  # 주거실태\n",
    "  abode_house_df = abode_house_df.drop(columns = '년도')\n",
    "\n",
    "  # 매매가격지수 (증감률)\n",
    "  priceRelative_df = priceRelative_df.drop(columns = '원자료')\n",
    "\n",
    "  # 서울시 면적\n",
    "  seoul_area_df\n",
    "\n",
    "  # 주택담보금리\n",
    "  rate_df.rename(columns = {'주택담보대출 (연리%)' : '금리'}, inplace = True)\n",
    "  rate_df[rate_df['연도'] == year]['금리']\n",
    "\n",
    "  dfs = [park_df, bus_df, train_df, develop_df,\n",
    "         demandSupply_df, distribute_df, hospital_df,\n",
    "         population_df, volume_df, abode_house_df, priceRelative_df,\n",
    "         seoul_area_df]  # 필요한 데이터프레임 추가\n",
    "  result = house_df.copy()\n",
    "  \n",
    "  ### 데이터 병합\n",
    "  for df in dfs : \n",
    "    # 병합\n",
    "    result = result.merge(df, on='구', how='left')\n",
    "\n",
    "  result['연도'] = year\n",
    "  result = result.merge(rate_df, on = '연도', how = 'left')\n",
    "  return result\n",
    "\n",
    "                    #######                            #######\n",
    "                    ##  데이터프레임 데이터 타입 변환 함수  ## (희만)\n",
    "                    #######                            #######\n",
    "def df_to_float(df) : \n",
    "  df['대형마트'] = df['대형마트'].astype(float)\n",
    "  df['백화점'] = df['백화점'].astype(float)\n",
    "  df['전문점'] = df['전문점'].astype(float)\n",
    "  df['쇼핑센터'] = df['쇼핑센터'].astype(float)\n",
    "  df['복합쇼핑몰'] = df['복합쇼핑몰'].astype(float)\n",
    "  return df\n",
    "\n",
    "                    #######                                           #######\n",
    "                    ##  아무것도 없는 컬럼 수만 동일한 데이터프레임 생성   ## (희만)\n",
    "                    #######                                           #######\n",
    "def copy_none_df( df) : \n",
    "  copy_df = df.copy()\n",
    "  column_names = copy_df.columns\n",
    "  for column in column_names :\n",
    "    copy_df[column] = np.nan\n",
    "  copy_df['구'] = df['구']\n",
    "  return copy_df\n",
    "\n",
    "                    #######                                            #######\n",
    "                    ##  컬럼별 연도에 따라 비어있는 행 상대적 평균치 적용   ## (희만)\n",
    "                    #######                                            #######\n",
    "def fillna_with_neighbor_mean(df, column, year_column):\n",
    "  \"\"\"\n",
    "  NaN 값을 인접 연도의 평균값으로 채우는 함수.\n",
    "  \n",
    "  Parameters:\n",
    "  - df: 데이터프레임\n",
    "  - column: NaN 값을 채울 컬럼 이름\n",
    "  - year_column: 연도를 나타내는 컬럼 이름\n",
    "  \n",
    "  Returns:\n",
    "  - NaN이 채워진 데이터프레임\n",
    "  \"\"\"\n",
    "  # NaN이 있는 연도와 없는 연도 분리\n",
    "  null_years = df.loc[df[column].isna(), year_column].unique()\n",
    "  not_null_years = df.loc[~df[column].isna(), year_column].unique()\n",
    "\n",
    "  # 연도별 평균값 계산\n",
    "  year_means = df.groupby(year_column)[column].mean()\n",
    "\n",
    "  # NaN 채우기\n",
    "  for null_year in null_years:\n",
    "    # null_year 보다 큰 not_null_year 중 가장 작은 연도\n",
    "    next_year = not_null_years[not_null_years > null_year].min() if (not_null_years > null_year).any() else None\n",
    "    # null_year 보다 작은 not_null_year 중 가장 큰 연도\n",
    "    prev_year = not_null_years[not_null_years < null_year].max() if (not_null_years < null_year).any() else None\n",
    "\n",
    "    # 평균값 계산\n",
    "    if prev_year is not None and next_year is not None:\n",
    "        fill_value = (year_means[prev_year] + year_means[next_year]) / 2\n",
    "    elif next_year is not None:\n",
    "        fill_value = year_means[next_year]\n",
    "    elif prev_year is not None:\n",
    "        fill_value = year_means[prev_year]\n",
    "    else:\n",
    "        fill_value = np.nan  # 채울 수 없는 경우 NaN 유지\n",
    "\n",
    "    # NaN 채우기\n",
    "    df.loc[(df[year_column] == null_year) & (df[column].isna()), column] = fill_value\n",
    "\n",
    "  return df\n",
    "\n",
    "                    #######                            #######\n",
    "                    ##  모델 학습 후 모델 성능 시각화 함수  ## (희만)\n",
    "                    #######                            #######\n",
    "def plot_model_performance(comparison_df, y_test, y_pred, model, X_train):\n",
    "  \"\"\"\n",
    "  Parameters:\n",
    "  - comparison_df: 실제 값과 예측 값 및 오차를 포함한 DataFrame\n",
    "  - y_test: 실제 값\n",
    "  - y_pred: 예측 값\n",
    "  - model: 학습된 모델 (예: RandomForestRegressor)\n",
    "  - X_train: 훈련 데이터 (특징)\n",
    "  \"\"\"\n",
    "  \n",
    "  # 시각화: 여러 그래프를 한 화면에 배치\n",
    "  fig, axs = plt.subplots(2, 2, figsize=(14, 10))  # 2x2 그리드로 서브플롯 설정\n",
    "\n",
    "  # 첫 번째 플롯: 실제 값과 예측 값 비교\n",
    "  axs[0, 0].plot(comparison_df['구'], comparison_df['평균시세(억)'], label=\"실제 평균시세\", marker='o', color='blue')\n",
    "  axs[0, 0].plot(comparison_df['구'], comparison_df['예측_평균시세'], label=\"예측 평균시세\", marker='o', color='orange')\n",
    "  for idx, row in comparison_df.iterrows():\n",
    "      axs[0, 0].plot([row['구'], row['구']], [row['평균시세(억)'], row['예측_평균시세']],\n",
    "                     linestyle='--', color='gray', alpha=0.5)\n",
    "  axs[0, 0].set_xticklabels(comparison_df['구'], rotation=45)\n",
    "  axs[0, 0].set_xlabel(\"구\")\n",
    "  axs[0, 0].set_ylabel(\"평균시세 (억)\")\n",
    "  axs[0, 0].set_title(\"실제 평균시세 vs 예측 평균시세\")\n",
    "  axs[0, 0].legend()\n",
    "  axs[0, 0].grid(alpha=0.3)\n",
    "\n",
    "  # 두 번째 플롯: 오차 분포\n",
    "  axs[0, 1].bar(comparison_df['구'], comparison_df['오차'], color='red', alpha=0.7)\n",
    "  axs[0, 1].set_xticklabels(comparison_df['구'], rotation=45)\n",
    "  axs[0, 1].set_xlabel(\"구\")\n",
    "  axs[0, 1].set_ylabel(\"오차 (억)\")\n",
    "  axs[0, 1].set_title(\"오차 분포\")\n",
    "  axs[0, 1].grid(alpha=0.3)\n",
    "\n",
    "  # 세 번째 플롯: 실제와 예측 값의 산점도\n",
    "  axs[1, 0].scatter(y_test, y_pred, alpha=0.7, color='blue')\n",
    "  axs[1, 0].plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle=\"--\")\n",
    "  axs[1, 0].set_xlabel(\"실제 값\")\n",
    "  axs[1, 0].set_ylabel(\"예측 값\")\n",
    "  axs[1, 0].set_title(\"실제 값 vs 예측 값 (산점도)\")\n",
    "  axs[1, 0].grid(alpha=0.3)\n",
    "\n",
    "  # 네 번째 플롯: 변수 중요도\n",
    "  importances = model.feature_importances_\n",
    "  features = X_train.columns  \n",
    "  axs[1, 1].barh(features, importances, color='skyblue')\n",
    "  axs[1, 1].set_title(\"독립변수 중요도\")\n",
    "  axs[1, 1].set_xlabel(\"중요도\")\n",
    "  axs[1, 1].set_ylabel(\"독립변수\")\n",
    "  axs[1, 1].grid(alpha=0.3)\n",
    "\n",
    "  # 레이아웃 조정\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069ecd80-baae-48da-876c-a721ce887e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "                              /////--     데이터 부족     --///// (희만)\n",
    "          버스    :    2015, 2016, 2017, 2018 없음    / 2019 ~ 2023 있음\n",
    "          지하철  :    2016, 2017, 2018, 2019 없음    / 2015, 2020 ~ 2023 있음\n",
    "\n",
    "          없는 데이터 Null 값으로 임의의 데이터프레임 삽입 후 평균치로 null값 처리할 예정\n",
    "'''\n",
    "bus_null_df = copy_none_df(bus_2021_df) # 컬럼명과 개수만 가져옴\n",
    "train_null_df = copy_none_df(train_2021_df) # 컬럼명과 개수만 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2831fb77-cdf5-4b56-8b27-d9d7b904e86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "                              /////--     데이터 병합     --///// (희만)\n",
    "          공원, 버스, 지하철, 부동산, 개발계획, 수요공급, 유통업체, 의료기관, 인구수, 거래량\n",
    "'''\n",
    "merge_year_2015 = merge_year_dataFrame(park_2015_df, bus_null_df, train_2015_df, house_2015_df,\n",
    "                                       develop_2015_df, demandSupply_2015_df, distribute_2015_df,\n",
    "                                       hospital_2015_df, population_2015_df, volume_2015_df,\n",
    "                                       abode_house_2015_df, priceRelative_2015_df, seoul_area_df, rate_df,\n",
    "                                       year = 2015)\n",
    "\n",
    "merge_year_2016 = merge_year_dataFrame(park_2016_df, bus_null_df, train_null_df, house_2016_df,\n",
    "                                       develop_2016_df, demandSupply_2016_df, distribute_2016_df,\n",
    "                                       hospital_2016_df, population_2016_df, volume_2016_df,\n",
    "                                       abode_house_2016_df, priceRelative_2016_df, seoul_area_df, rate_df,\n",
    "                                       year = 2016)\n",
    "\n",
    "merge_year_2017 = merge_year_dataFrame(park_2017_df, bus_null_df, train_null_df, house_2017_df,\n",
    "                                       develop_2017_df, demandSupply_2017_df, distribute_2017_df,\n",
    "                                       hospital_2017_df, population_2017_df, volume_2017_df,\n",
    "                                       abode_house_2017_df, priceRelative_2017_df, seoul_area_df, rate_df,\n",
    "                                       year = 2017)\n",
    "\n",
    "merge_year_2018 = merge_year_dataFrame(park_2018_df, bus_null_df, train_null_df, house_2018_df,\n",
    "                                       develop_2018_df, demandSupply_2018_df, distribute_2018_df,\n",
    "                                       hospital_2018_df, population_2018_df, volume_2018_df,\n",
    "                                       abode_house_2018_df, priceRelative_2018_df, seoul_area_df, rate_df,\n",
    "                                       year = 2018)\n",
    "\n",
    "merge_year_2019 = merge_year_dataFrame(park_2019_df, bus_2019_df, train_null_df, house_2019_df,\n",
    "                                       develop_2019_df, demandSupply_2019_df, distribute_2019_df,\n",
    "                                       hospital_2019_df, population_2019_df, volume_2019_df,\n",
    "                                       abode_house_2019_df, priceRelative_2019_df, seoul_area_df, rate_df,\n",
    "                                       year = 2019)\n",
    "\n",
    "merge_year_2020 = merge_year_dataFrame(park_2020_df, bus_2020_df, train_2020_df, house_2020_df,\n",
    "                                       develop_2020_df, demandSupply_2020_df, distribute_2020_df,\n",
    "                                       hospital_2020_df, population_2020_df, volume_2020_df,\n",
    "                                       abode_house_2020_df, priceRelative_2020_df, seoul_area_df, rate_df,\n",
    "                                       year = 2020)\n",
    "\n",
    "\n",
    "merge_year_2021 = merge_year_dataFrame(park_2021_df, bus_2021_df, train_2021_df, house_2021_df,\n",
    "                                       develop_2021_df, demandSupply_2021_df, distribute_2021_df,\n",
    "                                       hospital_2021_df, population_2021_df, volume_2021_df,\n",
    "                                       abode_house_2021_df, priceRelative_2021_df, seoul_area_df, rate_df,\n",
    "                                       year = 2021)\n",
    "\n",
    "merge_year_2022 = merge_year_dataFrame(park_2022_df, bus_2022_df, train_2022_df, house_2022_df,\n",
    "                                       develop_2022_df, demandSupply_2022_df, distribute_2022_df,\n",
    "                                       hospital_2022_df, population_2022_df, volume_2022_df,\n",
    "                                       abode_house_2022_df, priceRelative_2022_df, seoul_area_df, rate_df,\n",
    "                                       year = 2022)\n",
    "\n",
    "merge_year_2023 = merge_year_dataFrame(park_2023_df, bus_2023_df, train_2023_df, house_2023_df,\n",
    "                                       develop_2023_df, demandSupply_2023_df, distribute_2023_df,\n",
    "                                       hospital_2023_df, population_2023_df, volume_2023_df,\n",
    "                                       abode_house_2023_df, priceRelative_2023_df, seoul_area_df, rate_df,\n",
    "                                       year = 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ededd6-b363-496e-bee8-fee6990434f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "                              /////--     머신러닝용 데이터 결합     --///// (희만)\n",
    "'''\n",
    "concat_list = [merge_year_2015, merge_year_2016, merge_year_2017, merge_year_2018,\n",
    "               merge_year_2019, merge_year_2020, merge_year_2021, merge_year_2022,\n",
    "               merge_year_2023]\n",
    "ai_concat = pd.concat(concat_list, ignore_index = True)\n",
    "ai_concat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c18efd6-257c-4a7b-85aa-3fba47ed790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "결측치 처리 함수 실행 (희만)\n",
    "'''\n",
    "ai_concat = fillna_with_neighbor_mean(ai_concat, '버스 수', '연도')\n",
    "ai_concat = fillna_with_neighbor_mean(ai_concat, '역개수', '연도')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00d2973-8ef1-48b1-bad1-89c82d5849fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 데이터 타입 실수형으로 전환 (희만)\n",
    "ai_concat = df_to_float(ai_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d450e6a-e5e5-4521-8c5c-73ccd37f8344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평균시세 값을 억 단위로 변환 (희만)\n",
    "# 평균시세 값이 너무 크면 오차도 너무 크게 나오기 때문\n",
    "ai_concat['평균시세'] = round(ai_concat['평균시세'] / 10000, 6)\n",
    "\n",
    "# 컬럼명을 '평균시세()'으로 변경 (희만)\n",
    "ai_concat.rename(columns={'평균시세': '평균시세(억)',\n",
    "                          '계_면적 (㎡)' : '개발면적(㎡)',\n",
    "                          '합계' : '합계_유통업체',\n",
    "                          '내국인-계' : '내국인',\n",
    "                          '외국인-계' : '외국인',\n",
    "                          '계' : '합계_주택',\n",
    "                          '면적 (km²)' : '행정구별 면적 (km²)'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b720c9-ae38-4878-aca2-fbd2457f16d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "모델 최적화를 위한 파생변수 생성 (희만)\n",
    "'''\n",
    "\n",
    "# 파생변수 생성\n",
    "ai_concat[\"1인당_공원수\"] = ai_concat[\"합계_공원수\"] / ai_concat[\"총인구\"]\n",
    "# ai_concat.drop(columns = '합계_공원수', inplace=True)\n",
    "ai_concat[\"1인당_병원수\"] = ai_concat[\"병원수\"] / ai_concat[\"총인구\"]\n",
    "# ai_concat.drop(columns = '병원수', inplace=True)\n",
    "ai_concat[\"1인당_유통시설\"] = ai_concat[\"합계_유통업체\"] / ai_concat[\"총인구\"]\n",
    "# ai_concat.drop(columns = ['합계', '대형마트', '백화점', '전문점', '쇼핑센터', '복합쇼핑몰', '그밖의 대규모점포'], inplace=True)\n",
    "ai_concat['교통 접근성'] = (ai_concat['버스 수'] + ai_concat['역개수']) / ai_concat['총인구']\n",
    "ai_concat['총 인프라 수'] = (\n",
    "  ai_concat[\"버스 수\"] +\n",
    "  ai_concat[\"역개수\"] +\n",
    "  ai_concat['합계_유통업체'] +\n",
    "  ai_concat['병원수']\n",
    ")\n",
    "ai_concat['인프라 밀집도'] = ai_concat['총 인프라 수'] / ai_concat['행정구별 면적 (km²)']\n",
    "\n",
    "# 유통시설 밀도\n",
    "ai_concat['유통시설_밀도'] = ai_concat['합계_유통업체'] / ai_concat['총인구']\n",
    "\n",
    "# 병원수 대비 유통시설 비율\n",
    "ai_concat['병원_유통비'] = ai_concat['병원수'] / (ai_concat['합계_유통업체'] + 1)\n",
    "\n",
    "# 교통 접근성 * 병원수의 상호작용\n",
    "ai_concat['교통_병원'] = ai_concat['교통 접근성'] * ai_concat['병원수']\n",
    "\n",
    "ai_concat['주택 밀집도'] = round((ai_concat['합계_주택'] - ai_concat['비주거용주택']) / ai_concat['행정구별 면적 (km²)'], 2)\n",
    "\n",
    "# 1인당 거래량\n",
    "ai_concat['1인당_거래량'] = ai_concat['거래량'] / ai_concat['총인구']\n",
    "\n",
    "# 병원 밀집도\n",
    "ai_concat['병원_밀집도'] = ai_concat['병원수'] / ai_concat['행정구별 면적 (km²)']\n",
    "\n",
    "ai_concat['거래량 대비 개발계획'] = ai_concat['거래량'] / ai_concat['개발계획_합계']\n",
    "\n",
    "\n",
    "  ### 값이 큰 변수 로그 변환 ###\n",
    "ai_concat['log_거래량'] = np.log1p(ai_concat['거래량'])\n",
    "ai_concat['log_총인구'] = np.log1p(ai_concat['총인구'])\n",
    "ai_concat['log_합계_주택'] = np.log1p(ai_concat['합계_주택'] - ai_concat['비주거용주택'])\n",
    "\n",
    "\n",
    "# 불필요한 변수 제거\n",
    "ai_concat = ai_concat.drop(columns=[\"내국인\", \"외국인\"], errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762bca87-c33a-491a-a176-cd35117fc620",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_concat = ai_concat[['평균시세(억)', '구', '수요공급지수', '전기대비증감률', '연도', '1인당_유통시설', '교통 접근성', '총 인프라 수', '인프라 밀집도',\n",
    "       '병원_유통비', '교통_병원', '주택 밀집도', '병원_밀집도', '거래량 대비 개발계획', 'log_거래량',\n",
    "       'log_총인구', 'log_합계_주택', '금리']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ff0bf0-1eaa-4032-bd86-c3da7dffbc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''                                     이상치 제거 후\n",
    "              ///////    선택 1)   15 ~ 23 년도 데이터를 전부 섞은 후    ///////\n",
    "                   8 : 2 비율로 무작위로 나눠서 훈련용, 테스트용으로 분리하기\n",
    "'''\n",
    "######### 이상치 제거 (희만)\n",
    "### 적용해보기\n",
    "\n",
    "# 1. Q1, Q3 계산\n",
    "Q1 = ai_concat['평균시세(억)'].quantile(0.25)  # 1분위수\n",
    "Q3 = ai_concat['평균시세(억)'].quantile(0.75)  # 3분위수\n",
    "IQR = Q3 - Q1                           # IQR 계산\n",
    "\n",
    "# 2. 이상치 기준 설정\n",
    "lower_bound = Q1 - 1.5 * IQR            # 하한\n",
    "upper_bound = Q3 + 1.5 * IQR            # 상한\n",
    "\n",
    "# 3. 이상치 제거\n",
    "df_filtered = ai_concat[(ai_concat['평균시세(억)'] >= lower_bound) & (ai_concat['평균시세(억)'] <= upper_bound)]\n",
    "\n",
    "# 독립변수와 종속변수 분리\n",
    "X = df_filtered.drop(columns=['평균시세(억)', '구', '연도'])  # 종속변수와 제외할 컬럼 제거\n",
    "y = df_filtered['평균시세(억)']  # 종속변수\n",
    "\n",
    "# 데이터 분리: 80% 훈련용, 20% 테스트용\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 확인\n",
    "print(\"훈련 데이터 크기:\", X_train.shape, y_train.shape)\n",
    "print(\"테스트 데이터 크기:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea173ec-1a78-4a26-8b31-8a11e83a7f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Feature importances 확인\n",
    "# importances = model_xgb.feature_importances_\n",
    "\n",
    "# # 중요도가 0.001보다 작은 feature들의 컬럼명 출력\n",
    "# # X.columns를 사용하여 각 feature의 이름을 가져옴\n",
    "# small_importances = [feature for feature, importance in zip(X.columns, importances) if importance < 0.05]\n",
    "\n",
    "# print(\"0.001보다 작은 중요도를 가진 특성들:\")\n",
    "# '''\n",
    "# 0.001보다 작은 중요도를 가진 특성들:\n",
    "# ['대형마트', '유통시설_밀도', 'log_거래량', 'log_총인구', 'log_합계_주택']\n",
    "# '''\n",
    "# print(small_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3426ae-522a-4bb4-aa46-d7a16cbc8a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''                                                            (희만)\n",
    "                                       PCA 차원축소 후 그리드 서치로 최적의 파라미터를 찾은 후\n",
    "                            해당 파라미터를 적용 후 feature important 가 0.01 이하인 독립변수 제거 한 후 모델링 진행함\n",
    "\n",
    "            모델명 : model_xgb\n",
    "\n",
    "                                       \n",
    "'''\n",
    "\n",
    "# 1. 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. XGBRegressor 모델을 학습할 때 사용되는 최적의 파라미터\n",
    "best_params = {'colsample_bytree': 1.0,\n",
    "               'learning_rate': 0.05,\n",
    "               'max_depth': 7,\n",
    "               'n_estimators': 200,\n",
    "               'subsample': 0.8}\n",
    "model_xgb = XGBRegressor(**best_params)\n",
    "\n",
    "# 3. 모델 학습\n",
    "model_xgb.fit(X_train, y_train)\n",
    "\n",
    "# 4. 예측\n",
    "y_pred_xgb = model_xgb.predict(X_test)\n",
    "\n",
    "# 5. 모델 평가\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "# 평가 지표 출력\n",
    "print(f\"Best XGBoost Mean Squared Error: {mse_xgb:.2f}\")\n",
    "print(f\"Best XGBoost R2 Score: {r2_xgb:.2f}\")\n",
    "\n",
    "# 6. 실제 데이터, 예측 데이터, 오차 계산\n",
    "comparison_df_xgb = pd.DataFrame({\n",
    "    '구': ai_concat.iloc[X_test.index]['구'],\n",
    "    '평균시세(억)': y_test.values,\n",
    "    '예측_평균시세': y_pred_xgb\n",
    "})\n",
    "\n",
    "# 오차 계산\n",
    "comparison_df_xgb['오차'] = np.abs(comparison_df_xgb['평균시세(억)'] - comparison_df_xgb['예측_평균시세'])\n",
    "comparison_df_xgb = comparison_df_xgb.sort_values('구')\n",
    "display(comparison_df_xgb)\n",
    "\n",
    "# 예측 결과와 실제 결과의 크기 확인\n",
    "print(f\"y_test size: {y_test.shape}\")\n",
    "print(f\"y_pred size: {y_pred_xgb.shape}\")\n",
    "\n",
    "# 7. 인덱스 재조정: X_test의 인덱스를 y_test와 맞추기\n",
    "comparison_df_xgb = pd.DataFrame({\n",
    "    '구': ai_concat.iloc[X_test.index]['구'],  # X_test와 관련된 인덱스 사용\n",
    "    '평균시세(억)': y_test.values,\n",
    "    '예측_평균시세': y_pred_xgb\n",
    "})\n",
    "\n",
    "# 오차 계산\n",
    "comparison_df_xgb['오차'] = np.abs(comparison_df_xgb['평균시세(억)'] - comparison_df_xgb['예측_평균시세'])\n",
    "comparison_df_xgb = comparison_df_xgb.sort_values('구')\n",
    "\n",
    "# 8. 시각화 함수 호출\n",
    "plot_model_performance(comparison_df_xgb, y_test, y_pred_xgb, model_xgb, X_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
