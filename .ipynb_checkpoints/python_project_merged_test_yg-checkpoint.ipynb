{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296e1658-8f17-4126-828c-0d30f053e041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import wordcloud\n",
    "from wordcloud import WordCloud\n",
    "import json\n",
    "import folium\n",
    "%matplotlib inline\n",
    "# 한글 설정\n",
    "# pip install koreanize_matplotlib\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "root = 'C:/workspace/python/project/data/'\n",
    "\n",
    "# 전처리 완료한 데이터파일 이동경로\n",
    "pre_root = 'C:/workspace/python/project/data/_전처리/'\n",
    "\n",
    "# 구글드라이브 : https://drive.google.com/drive/folders/1zIzm1o8-3uxcWSU2DoWpB8aV0Oxdfz_P?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456056dd-5c73-4677-92d7-3d7fb89f5843",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "                         /////--     데이터 불러오기     --/////\n",
    "'''\n",
    "# 공원\n",
    "park_2015_df = pd.read_csv(pre_root + '공원_Data/전처리_공원수_2015.csv', encoding = 'cp949')\n",
    "park_2016_df = pd.read_csv(pre_root + '공원_Data/전처리_공원수_2016.csv', encoding = 'cp949')\n",
    "park_2017_df = pd.read_csv(pre_root + '공원_Data/전처리_공원수_2017.csv', encoding = 'cp949')\n",
    "park_2018_df = pd.read_csv(pre_root + '공원_Data/전처리_공원수_2018.csv', encoding = 'cp949')\n",
    "park_2019_df = pd.read_csv(pre_root + '공원_Data/전처리_공원수_2019.csv', encoding = 'cp949')\n",
    "park_2020_df = pd.read_csv(pre_root + '공원_Data/전처리_공원수_2020.csv', encoding = 'cp949')\n",
    "\n",
    "park_2021_df = pd.read_csv(pre_root + '공원_Data/전처리_공원수_2021.csv', encoding = 'cp949')\n",
    "park_2022_df = pd.read_csv(pre_root + '공원_Data/전처리_공원수_2022.csv', encoding = 'cp949')\n",
    "park_2023_df = pd.read_csv(pre_root + '공원_Data/전처리_공원수_2023.csv', encoding = 'cp949')\n",
    "\n",
    "# 교통\n",
    "## 버스\n",
    "bus_2019_df = pd.read_csv(pre_root + '교통_Data/전처리_버스수_2019.csv', encoding = 'cp949')\n",
    "bus_2020_df = pd.read_csv(pre_root + '교통_Data/전처리_버스수_2020.csv', encoding = 'cp949')\n",
    "\n",
    "bus_2021_df = pd.read_csv(pre_root + '교통_Data/전처리_버스수_2021.csv', encoding = 'cp949')\n",
    "bus_2022_df = pd.read_csv(pre_root + '교통_Data/전처리_버스수_2022.csv', encoding = 'cp949')\n",
    "bus_2023_df = pd.read_csv(pre_root + '교통_Data/전처리_버스수_2023.csv', encoding = 'cp949')\n",
    "## 지하철\n",
    "train_2015_df = pd.read_csv(pre_root + '교통_Data/전처리_지하철수_2015.csv', encoding = 'cp949')\n",
    "train_2020_df = pd.read_csv(pre_root + '교통_Data/전처리_지하철수_2020.csv', encoding = 'cp949')\n",
    "\n",
    "train_2021_df = pd.read_csv(pre_root + '교통_Data/전처리_지하철수_2021.csv', encoding = 'cp949')\n",
    "train_2022_df = pd.read_csv(pre_root + '교통_Data/전처리_지하철수_2022.csv', encoding = 'cp949')\n",
    "train_2023_df = pd.read_csv(pre_root + '교통_Data/전처리_지하철수_2023.csv', encoding = 'cp949')\n",
    "\n",
    "# 부동산\n",
    "## 서울시 집값\n",
    "house_2015_df = pd.read_csv(pre_root + '부동산_Data/전처리_부동산_2015.csv', encoding = 'cp949')\n",
    "house_2016_df = pd.read_csv(pre_root + '부동산_Data/전처리_부동산_2016.csv', encoding = 'cp949')\n",
    "house_2017_df = pd.read_csv(pre_root + '부동산_Data/전처리_부동산_2017.csv', encoding = 'cp949')\n",
    "house_2018_df = pd.read_csv(pre_root + '부동산_Data/전처리_부동산_2018.csv', encoding = 'cp949')\n",
    "house_2019_df = pd.read_csv(pre_root + '부동산_Data/전처리_부동산_2019.csv', encoding = 'cp949')\n",
    "house_2020_df = pd.read_csv(pre_root + '부동산_Data/전처리_부동산_2020.csv', encoding = 'cp949')\n",
    "\n",
    "house_2021_df = pd.read_csv(pre_root + '부동산_Data/전처리_부동산_2021.csv', encoding = 'cp949')\n",
    "house_2022_df = pd.read_csv(pre_root + '부동산_Data/전처리_부동산_2022.csv', encoding = 'cp949')\n",
    "house_2023_df = pd.read_csv(pre_root + '부동산_Data/전처리_부동산_2023.csv', encoding = 'cp949')\n",
    "## 개발계획\n",
    "develop_2015_df = pd.read_csv(pre_root + '부동산_Data/전처리_개발계획_2015.csv', encoding = 'cp949')\n",
    "develop_2016_df = pd.read_csv(pre_root + '부동산_Data/전처리_개발계획_2016.csv', encoding = 'cp949')\n",
    "develop_2017_df = pd.read_csv(pre_root + '부동산_Data/전처리_개발계획_2017.csv', encoding = 'cp949')\n",
    "develop_2018_df = pd.read_csv(pre_root + '부동산_Data/전처리_개발계획_2018.csv', encoding = 'cp949')\n",
    "develop_2019_df = pd.read_csv(pre_root + '부동산_Data/전처리_개발계획_2019.csv', encoding = 'cp949')\n",
    "develop_2020_df = pd.read_csv(pre_root + '부동산_Data/전처리_개발계획_2020.csv', encoding = 'cp949')\n",
    "\n",
    "develop_2021_df = pd.read_csv(pre_root + '부동산_Data/전처리_개발계획_2021.csv', encoding = 'cp949')\n",
    "develop_2022_df = pd.read_csv(pre_root + '부동산_Data/전처리_개발계획_2022.csv', encoding = 'cp949')\n",
    "develop_2023_df = pd.read_csv(pre_root + '부동산_Data/전처리_개발계획_2023.csv', encoding = 'cp949')\n",
    "\n",
    "# 수요공급\n",
    "demandSupply_2015_df = pd.read_csv(pre_root + '수요공급_Data/전처리_수요공급지수_2015.csv', encoding = 'cp949')\n",
    "demandSupply_2016_df = pd.read_csv(pre_root + '수요공급_Data/전처리_수요공급지수_2016.csv', encoding = 'cp949')\n",
    "demandSupply_2017_df = pd.read_csv(pre_root + '수요공급_Data/전처리_수요공급지수_2017.csv', encoding = 'cp949')\n",
    "demandSupply_2018_df = pd.read_csv(pre_root + '수요공급_Data/전처리_수요공급지수_2018.csv', encoding = 'cp949')\n",
    "demandSupply_2019_df = pd.read_csv(pre_root + '수요공급_Data/전처리_수요공급지수_2019.csv', encoding = 'cp949')\n",
    "demandSupply_2020_df = pd.read_csv(pre_root + '수요공급_Data/전처리_수요공급지수_2020.csv', encoding = 'cp949')\n",
    "\n",
    "demandSupply_2021_df = pd.read_csv(pre_root + '수요공급_Data/전처리_수요공급지수_2021.csv', encoding = 'cp949')\n",
    "demandSupply_2022_df = pd.read_csv(pre_root + '수요공급_Data/전처리_수요공급지수_2022.csv', encoding = 'cp949')\n",
    "demandSupply_2023_df = pd.read_csv(pre_root + '수요공급_Data/전처리_수요공급지수_2023.csv', encoding = 'cp949')\n",
    "\n",
    "# 유통업체\n",
    "distribute_2015_df = pd.read_csv(pre_root + '유통업체_Data/전처리_유통업체_2015.csv', encoding = 'cp949')\n",
    "distribute_2016_df = pd.read_csv(pre_root + '유통업체_Data/전처리_유통업체_2016.csv', encoding = 'cp949')\n",
    "distribute_2017_df = pd.read_csv(pre_root + '유통업체_Data/전처리_유통업체_2017.csv', encoding = 'cp949')\n",
    "distribute_2018_df = pd.read_csv(pre_root + '유통업체_Data/전처리_유통업체_2018.csv', encoding = 'cp949')\n",
    "distribute_2019_df = pd.read_csv(pre_root + '유통업체_Data/전처리_유통업체_2019.csv', encoding = 'cp949')\n",
    "distribute_2020_df = pd.read_csv(pre_root + '유통업체_Data/전처리_유통업체_2020.csv', encoding = 'cp949')\n",
    "\n",
    "distribute_2021_df = pd.read_csv(pre_root + '유통업체_Data/전처리_유통업체_2021.csv', encoding = 'cp949')\n",
    "distribute_2022_df = pd.read_csv(pre_root + '유통업체_Data/전처리_유통업체_2022.csv', encoding = 'cp949')\n",
    "distribute_2023_df = pd.read_csv(pre_root + '유통업체_Data/전처리_유통업체_2023.csv', encoding = 'cp949')\n",
    "\n",
    "# 의료기관\n",
    "hospital_2015_df = pd.read_csv(pre_root + '의료기관_Data/전처리_병원수_2015.csv', encoding = 'cp949')\n",
    "hospital_2016_df = pd.read_csv(pre_root + '의료기관_Data/전처리_병원수_2016.csv', encoding = 'cp949')\n",
    "hospital_2017_df = pd.read_csv(pre_root + '의료기관_Data/전처리_병원수_2017.csv', encoding = 'cp949')\n",
    "hospital_2018_df = pd.read_csv(pre_root + '의료기관_Data/전처리_병원수_2018.csv', encoding = 'cp949')\n",
    "hospital_2019_df = pd.read_csv(pre_root + '의료기관_Data/전처리_병원수_2019.csv', encoding = 'cp949')\n",
    "hospital_2020_df = pd.read_csv(pre_root + '의료기관_Data/전처리_병원수_2020.csv', encoding = 'cp949')\n",
    "\n",
    "hospital_2021_df = pd.read_csv(pre_root + '의료기관_Data/전처리_병원수_2021.csv', encoding = 'cp949')\n",
    "hospital_2022_df = pd.read_csv(pre_root + '의료기관_Data/전처리_병원수_2022.csv', encoding = 'cp949')\n",
    "hospital_2023_df = pd.read_csv(pre_root + '의료기관_Data/전처리_병원수_2023.csv', encoding = 'cp949')\n",
    "\n",
    "# 인구수\n",
    "population_2015_df = pd.read_csv(pre_root + '인구수_Data/전처리_인구수_2015.csv', encoding = 'cp949')\n",
    "population_2016_df = pd.read_csv(pre_root + '인구수_Data/전처리_인구수_2016.csv', encoding = 'cp949')\n",
    "population_2017_df = pd.read_csv(pre_root + '인구수_Data/전처리_인구수_2017.csv', encoding = 'cp949')\n",
    "population_2018_df = pd.read_csv(pre_root + '인구수_Data/전처리_인구수_2018.csv', encoding = 'cp949')\n",
    "population_2019_df = pd.read_csv(pre_root + '인구수_Data/전처리_인구수_2019.csv', encoding = 'cp949')\n",
    "population_2020_df = pd.read_csv(pre_root + '인구수_Data/전처리_인구수_2020.csv', encoding = 'cp949')\n",
    "\n",
    "population_2021_df = pd.read_csv(pre_root + '인구수_Data/전처리_인구수_2021.csv', encoding = 'cp949')\n",
    "population_2022_df = pd.read_csv(pre_root + '인구수_Data/전처리_인구수_2022.csv', encoding = 'cp949')\n",
    "population_2023_df = pd.read_csv(pre_root + '인구수_Data/전처리_인구수_2023.csv', encoding = 'cp949')\n",
    "\n",
    "# 주거실태 (거래량)\n",
    "volume_2015_df = pd.read_csv(pre_root + '주거실태_Data/전처리_거래량_2015.csv', encoding = 'cp949')\n",
    "volume_2016_df = pd.read_csv(pre_root + '주거실태_Data/전처리_거래량_2016.csv', encoding = 'cp949')\n",
    "volume_2017_df = pd.read_csv(pre_root + '주거실태_Data/전처리_거래량_2017.csv', encoding = 'cp949')\n",
    "volume_2018_df = pd.read_csv(pre_root + '주거실태_Data/전처리_거래량_2018.csv', encoding = 'cp949')\n",
    "volume_2019_df = pd.read_csv(pre_root + '주거실태_Data/전처리_거래량_2019.csv', encoding = 'cp949')\n",
    "volume_2020_df = pd.read_csv(pre_root + '주거실태_Data/전처리_거래량_2020.csv', encoding = 'cp949')\n",
    "\n",
    "volume_2021_df = pd.read_csv(pre_root + '주거실태_Data/전처리_거래량_2021.csv', encoding = 'cp949')\n",
    "volume_2022_df = pd.read_csv(pre_root + '주거실태_Data/전처리_거래량_2022.csv', encoding = 'cp949')\n",
    "volume_2023_df = pd.read_csv(pre_root + '주거실태_Data/전처리_거래량_2023.csv', encoding = 'cp949')\n",
    "\n",
    "# 주거실태\n",
    "abode_house_2015_df = pd.read_csv(pre_root + '주거실태_Data/전처리_주거실태_2015.csv', encoding = 'cp949')\n",
    "abode_house_2016_df = pd.read_csv(pre_root + '주거실태_Data/전처리_주거실태_2016.csv', encoding = 'cp949')\n",
    "abode_house_2017_df = pd.read_csv(pre_root + '주거실태_Data/전처리_주거실태_2017.csv', encoding = 'cp949')\n",
    "abode_house_2018_df = pd.read_csv(pre_root + '주거실태_Data/전처리_주거실태_2018.csv', encoding = 'cp949')\n",
    "abode_house_2019_df = pd.read_csv(pre_root + '주거실태_Data/전처리_주거실태_2019.csv', encoding = 'cp949')\n",
    "abode_house_2020_df = pd.read_csv(pre_root + '주거실태_Data/전처리_주거실태_2020.csv', encoding = 'cp949')\n",
    "\n",
    "abode_house_2021_df = pd.read_csv(pre_root + '주거실태_Data/전처리_주거실태_2021.csv', encoding = 'cp949')\n",
    "abode_house_2022_df = pd.read_csv(pre_root + '주거실태_Data/전처리_주거실태_2022.csv', encoding = 'cp949')\n",
    "abode_house_2023_df = pd.read_csv(pre_root + '주거실태_Data/전처리_주거실태_2023.csv', encoding = 'cp949')\n",
    "\n",
    "# 매매가격지수\n",
    "priceRelative_2015_df = pd.read_csv(pre_root + '수요공급_Data/전처리_매매가격지수_2015.csv', encoding = 'cp949')\n",
    "priceRelative_2016_df = pd.read_csv(pre_root + '수요공급_Data/전처리_매매가격지수_2016.csv', encoding = 'cp949')\n",
    "priceRelative_2017_df = pd.read_csv(pre_root + '수요공급_Data/전처리_매매가격지수_2017.csv', encoding = 'cp949')\n",
    "priceRelative_2018_df = pd.read_csv(pre_root + '수요공급_Data/전처리_매매가격지수_2018.csv', encoding = 'cp949')\n",
    "priceRelative_2019_df = pd.read_csv(pre_root + '수요공급_Data/전처리_매매가격지수_2019.csv', encoding = 'cp949')\n",
    "priceRelative_2020_df = pd.read_csv(pre_root + '수요공급_Data/전처리_매매가격지수_2020.csv', encoding = 'cp949')\n",
    "\n",
    "priceRelative_2021_df = pd.read_csv(pre_root + '수요공급_Data/전처리_매매가격지수_2021.csv', encoding = 'cp949')\n",
    "priceRelative_2022_df = pd.read_csv(pre_root + '수요공급_Data/전처리_매매가격지수_2022.csv', encoding = 'cp949')\n",
    "priceRelative_2023_df = pd.read_csv(pre_root + '수요공급_Data/전처리_매매가격지수_2023.csv', encoding = 'cp949')\n",
    "\n",
    "# 서울시 면적\n",
    "seoul_area_df = pd.read_csv(pre_root + '서울시_자치구/전처리_면적_2023.csv', encoding = 'cp949')\n",
    "\n",
    "print('데이터 프레임 삽입 완료..!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51550a1e-0c2e-48bd-95e5-afc98a446e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "                         /////--     데이터 병합 함수     --/////\n",
    "          공원, 버스, 지하철, 부동산, 개발계획, 수요공급, 유통업체, 의료기관, 인구수, 거래량\n",
    "'''\n",
    "def merge_year_dataFrame(park_df, bus_df, train_df, house_df, develop_df, demandSupply_df,\n",
    "                         distribute_df, hospital_df, population_df, volume_df,\n",
    "                         abode_house_df, priceRelative_df, seoul_area_df, year) :\n",
    "  # 공원\n",
    "  park_df = park_df[['구', '합계_공원수 (개소)']].rename(columns = {'합계_공원수 (개소)' : '합계_공원수'})\n",
    "  \n",
    "  # 버스\n",
    "  if bus_df.drop(columns=['구'], axis=1).isnull().all().all() : \n",
    "    bus_df = pd.DataFrame({'구' : house_2021_df['구'], '버스 수' : np.nan})  \n",
    "  else : \n",
    "    bus_df = bus_df['구'].value_counts().reset_index().rename(columns = {'count' : '버스 수'})\n",
    "  \n",
    "  # 지하철\n",
    "  train_df = train_df[['구', '역개수']]\n",
    "  \n",
    "  # 부동산\n",
    "  house_df = house_df[['구', '평균시세']]\n",
    "  \n",
    "  # 개발계획\n",
    "  develop_df = develop_df[['구', '계_구역수 (개)', '계_면적 (㎡)']].rename(columns = {'계_구역수 (개)' : '개발계획_합계'})\n",
    "  \n",
    "  # 수요공급\n",
    "  demandSupply_df = round(demandSupply_df.groupby('구')['수급등급'].mean(), 2).reset_index().rename(columns = {'수급등급' : '수요공급지수'})\n",
    "  \n",
    "  # 유통업체\n",
    "  distribute_df = distribute_df.iloc[1:, :].reset_index(drop = True)\n",
    "  condition = distribute_df.columns.str.contains('_개소') # 원하는 컬럼만 조회\n",
    "  condition[0] = True\n",
    "  distribute_df = distribute_df.loc[:, condition]\n",
    "  column_names = {} # 컬럼 이름 변경용 딕셔너리\n",
    "  columns = distribute_df.columns\n",
    "  for column in columns :\n",
    "    column_names[column] = column.split('_개소')[0]\n",
    "  distribute_df = distribute_df.rename(columns = column_names)\n",
    "  distribute_df = distribute_df.replace('-', 0)\n",
    "  \n",
    "  # 의료기관\n",
    "  hospital_df = hospital_df[['구', '소계_병원수']].rename(columns = {'소계_병원수' : '병원수'})\n",
    "  hospital_df = hospital_df.iloc[1:, :].reset_index(drop = True)\n",
    "  \n",
    "  # 인구수\n",
    "  population_df\n",
    "  \n",
    "  # 주거실태 (거래량)\n",
    "  volume_df = volume_df.drop(columns = '년도').rename(columns = {'동(호)수' : '거래량'})[['구', '거래량']]\n",
    "\n",
    "  # 주거실태\n",
    "  abode_house_df = abode_house_df.drop(columns = '년도')\n",
    "\n",
    "  # 매매가격지수 (증감률)\n",
    "  priceRelative_df = priceRelative_df.drop(columns = '원자료')\n",
    "\n",
    "  # 서울시 면적\n",
    "  seoul_area_df\n",
    "\n",
    "  dfs = [park_df, bus_df, train_df, develop_df,\n",
    "         demandSupply_df, distribute_df, hospital_df,\n",
    "         population_df, volume_df, abode_house_df, priceRelative_df,\n",
    "         seoul_area_df]  # 필요한 데이터프레임 추가\n",
    "  result = house_df.copy()\n",
    "  \n",
    "  ### 데이터 병합\n",
    "  for df in dfs : \n",
    "    # 병합\n",
    "    result = result.merge(df, on='구', how='left')\n",
    "\n",
    "  result['연도'] = year\n",
    "\n",
    "  return result\n",
    "\n",
    "                    #######                            #######\n",
    "                    ##  데이터프레임 데이터 타입 변환 함수  ##\n",
    "                    #######                            #######\n",
    "def df_to_float(df) : \n",
    "  df['대형마트'] = df['대형마트'].astype(float)\n",
    "  df['백화점'] = df['백화점'].astype(float)\n",
    "  df['전문점'] = df['전문점'].astype(float)\n",
    "  df['쇼핑센터'] = df['쇼핑센터'].astype(float)\n",
    "  df['복합쇼핑몰'] = df['복합쇼핑몰'].astype(float)\n",
    "  return df\n",
    "\n",
    "                    #######                                           #######\n",
    "                    ##  아무것도 없는 컬럼 수만 동일한 데이터프레임 생성   ##\n",
    "                    #######                                           #######\n",
    "def copy_none_df( df) : \n",
    "  copy_df = df.copy()\n",
    "  column_names = copy_df.columns\n",
    "  for column in column_names :\n",
    "    copy_df[column] = np.nan\n",
    "  copy_df['구'] = df['구']\n",
    "  return copy_df\n",
    "\n",
    "                    #######                                            #######\n",
    "                    ##  컬럼별 연도에 따라 비어있는 행 상대적 평균치 적용   ##\n",
    "                    #######                                            #######\n",
    "def fillna_with_neighbor_mean(df, column, year_column):\n",
    "  \"\"\"\n",
    "  NaN 값을 인접 연도의 평균값으로 채우는 함수.\n",
    "  \n",
    "  Parameters:\n",
    "  - df: 데이터프레임\n",
    "  - column: NaN 값을 채울 컬럼 이름\n",
    "  - year_column: 연도를 나타내는 컬럼 이름\n",
    "  \n",
    "  Returns:\n",
    "  - NaN이 채워진 데이터프레임\n",
    "  \"\"\"\n",
    "  # NaN이 있는 연도와 없는 연도 분리\n",
    "  null_years = df.loc[df[column].isna(), year_column].unique()\n",
    "  not_null_years = df.loc[~df[column].isna(), year_column].unique()\n",
    "\n",
    "  # 연도별 평균값 계산\n",
    "  year_means = df.groupby(year_column)[column].mean()\n",
    "\n",
    "  # NaN 채우기\n",
    "  for null_year in null_years:\n",
    "    # null_year 보다 큰 not_null_year 중 가장 작은 연도\n",
    "    next_year = not_null_years[not_null_years > null_year].min() if (not_null_years > null_year).any() else None\n",
    "    # null_year 보다 작은 not_null_year 중 가장 큰 연도\n",
    "    prev_year = not_null_years[not_null_years < null_year].max() if (not_null_years < null_year).any() else None\n",
    "\n",
    "    # 평균값 계산\n",
    "    if prev_year is not None and next_year is not None:\n",
    "        fill_value = (year_means[prev_year] + year_means[next_year]) / 2\n",
    "    elif next_year is not None:\n",
    "        fill_value = year_means[next_year]\n",
    "    elif prev_year is not None:\n",
    "        fill_value = year_means[prev_year]\n",
    "    else:\n",
    "        fill_value = np.nan  # 채울 수 없는 경우 NaN 유지\n",
    "\n",
    "    # NaN 채우기\n",
    "    df.loc[(df[year_column] == null_year) & (df[column].isna()), column] = fill_value\n",
    "\n",
    "  return df\n",
    "\n",
    "                    #######                            #######\n",
    "                    ##  모델 학습 후 모델 성능 시각화 함수  ##\n",
    "                    #######                            #######\n",
    "def plot_model_performance(comparison_df, y_test, y_pred, model, X_train):\n",
    "  \"\"\"\n",
    "  Parameters:\n",
    "  - comparison_df: 실제 값과 예측 값 및 오차를 포함한 DataFrame\n",
    "  - y_test: 실제 값\n",
    "  - y_pred: 예측 값\n",
    "  - model: 학습된 모델 (예: RandomForestRegressor)\n",
    "  - X_train: 훈련 데이터 (특징)\n",
    "  \"\"\"\n",
    "  \n",
    "  # 시각화: 여러 그래프를 한 화면에 배치\n",
    "  fig, axs = plt.subplots(2, 2, figsize=(14, 10))  # 2x2 그리드로 서브플롯 설정\n",
    "\n",
    "  # 첫 번째 플롯: 실제 값과 예측 값 비교\n",
    "  axs[0, 0].plot(comparison_df['구'], comparison_df['평균시세(억)'], label=\"실제 평균시세\", marker='o', color='blue')\n",
    "  axs[0, 0].plot(comparison_df['구'], comparison_df['예측_평균시세'], label=\"예측 평균시세\", marker='o', color='orange')\n",
    "  for idx, row in comparison_df.iterrows():\n",
    "      axs[0, 0].plot([row['구'], row['구']], [row['평균시세(억)'], row['예측_평균시세']],\n",
    "                     linestyle='--', color='gray', alpha=0.5)\n",
    "  axs[0, 0].set_xticklabels(comparison_df['구'], rotation=45)\n",
    "  axs[0, 0].set_xlabel(\"구\")\n",
    "  axs[0, 0].set_ylabel(\"평균시세 (억)\")\n",
    "  axs[0, 0].set_title(\"실제 평균시세 vs 예측 평균시세\")\n",
    "  axs[0, 0].legend()\n",
    "  axs[0, 0].grid(alpha=0.3)\n",
    "\n",
    "  # 두 번째 플롯: 오차 분포\n",
    "  axs[0, 1].bar(comparison_df['구'], comparison_df['오차'], color='red', alpha=0.7)\n",
    "  axs[0, 1].set_xticklabels(comparison_df['구'], rotation=45)\n",
    "  axs[0, 1].set_xlabel(\"구\")\n",
    "  axs[0, 1].set_ylabel(\"오차 (억)\")\n",
    "  axs[0, 1].set_title(\"오차 분포\")\n",
    "  axs[0, 1].grid(alpha=0.3)\n",
    "\n",
    "  # 세 번째 플롯: 실제와 예측 값의 산점도\n",
    "  axs[1, 0].scatter(y_test, y_pred, alpha=0.7, color='blue')\n",
    "  axs[1, 0].plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle=\"--\")\n",
    "  axs[1, 0].set_xlabel(\"실제 값\")\n",
    "  axs[1, 0].set_ylabel(\"예측 값\")\n",
    "  axs[1, 0].set_title(\"실제 값 vs 예측 값 (산점도)\")\n",
    "  axs[1, 0].grid(alpha=0.3)\n",
    "\n",
    "  # 네 번째 플롯: 변수 중요도\n",
    "  importances = model.feature_importances_\n",
    "  features = X_train.columns  \n",
    "  axs[1, 1].barh(features, importances, color='skyblue')\n",
    "  axs[1, 1].set_title(\"독립변수 중요도\")\n",
    "  axs[1, 1].set_xlabel(\"중요도\")\n",
    "  axs[1, 1].set_ylabel(\"독립변수\")\n",
    "  axs[1, 1].grid(alpha=0.3)\n",
    "\n",
    "  # 레이아웃 조정\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069ecd80-baae-48da-876c-a721ce887e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "                              /////--     데이터 부족     --/////\n",
    "          버스    :    2015, 2016, 2017, 2018 없음    / 2019 ~ 2023 있음\n",
    "          지하철  :    2016, 2017, 2018, 2019 없음    / 2015, 2020 ~ 2023 있음\n",
    "\n",
    "          없는 데이터 Null 값으로 임의의 데이터프레임 삽입 후 평균치로 null값 처리할 예정\n",
    "'''\n",
    "bus_null_df = copy_none_df(bus_2021_df) # 컬럼명과 개수만 가져옴\n",
    "train_null_df = copy_none_df(train_2021_df) # 컬럼명과 개수만 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2831fb77-cdf5-4b56-8b27-d9d7b904e86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "                              /////--     데이터 병합     --/////\n",
    "          공원, 버스, 지하철, 부동산, 개발계획, 수요공급, 유통업체, 의료기관, 인구수, 거래량\n",
    "'''\n",
    "merge_year_2015 = merge_year_dataFrame(park_2015_df, bus_null_df, train_2015_df, house_2015_df,\n",
    "                                       develop_2015_df, demandSupply_2015_df, distribute_2015_df,\n",
    "                                       hospital_2015_df, population_2015_df, volume_2015_df,\n",
    "                                       abode_house_2015_df, priceRelative_2015_df, seoul_area_df,\n",
    "                                       year = 2015)\n",
    "\n",
    "merge_year_2016 = merge_year_dataFrame(park_2016_df, bus_null_df, train_null_df, house_2016_df,\n",
    "                                       develop_2016_df, demandSupply_2016_df, distribute_2016_df,\n",
    "                                       hospital_2016_df, population_2016_df, volume_2016_df,\n",
    "                                       abode_house_2016_df, priceRelative_2016_df, seoul_area_df,\n",
    "                                       year = 2016)\n",
    "\n",
    "merge_year_2017 = merge_year_dataFrame(park_2017_df, bus_null_df, train_null_df, house_2017_df,\n",
    "                                       develop_2017_df, demandSupply_2017_df, distribute_2017_df,\n",
    "                                       hospital_2017_df, population_2017_df, volume_2017_df,\n",
    "                                       abode_house_2017_df, priceRelative_2017_df, seoul_area_df,\n",
    "                                       year = 2017)\n",
    "\n",
    "merge_year_2018 = merge_year_dataFrame(park_2018_df, bus_null_df, train_null_df, house_2018_df,\n",
    "                                       develop_2018_df, demandSupply_2018_df, distribute_2018_df,\n",
    "                                       hospital_2018_df, population_2018_df, volume_2018_df,\n",
    "                                       abode_house_2018_df, priceRelative_2018_df, seoul_area_df,\n",
    "                                       year = 2018)\n",
    "\n",
    "merge_year_2019 = merge_year_dataFrame(park_2019_df, bus_2019_df, train_null_df, house_2019_df,\n",
    "                                       develop_2019_df, demandSupply_2019_df, distribute_2019_df,\n",
    "                                       hospital_2019_df, population_2019_df, volume_2019_df,\n",
    "                                       abode_house_2019_df, priceRelative_2019_df, seoul_area_df,\n",
    "                                       year = 2019)\n",
    "\n",
    "merge_year_2020 = merge_year_dataFrame(park_2020_df, bus_2020_df, train_2020_df, house_2020_df,\n",
    "                                       develop_2020_df, demandSupply_2020_df, distribute_2020_df,\n",
    "                                       hospital_2020_df, population_2020_df, volume_2020_df,\n",
    "                                       abode_house_2020_df, priceRelative_2020_df, seoul_area_df,\n",
    "                                       year = 2020)\n",
    "\n",
    "\n",
    "merge_year_2021 = merge_year_dataFrame(park_2021_df, bus_2021_df, train_2021_df, house_2021_df,\n",
    "                                       develop_2021_df, demandSupply_2021_df, distribute_2021_df,\n",
    "                                       hospital_2021_df, population_2021_df, volume_2021_df,\n",
    "                                       abode_house_2021_df, priceRelative_2021_df, seoul_area_df,\n",
    "                                       year = 2021)\n",
    "\n",
    "merge_year_2022 = merge_year_dataFrame(park_2022_df, bus_2022_df, train_2022_df, house_2022_df,\n",
    "                                       develop_2022_df, demandSupply_2022_df, distribute_2022_df,\n",
    "                                       hospital_2022_df, population_2022_df, volume_2022_df,\n",
    "                                       abode_house_2022_df, priceRelative_2022_df, seoul_area_df,\n",
    "                                       year = 2022)\n",
    "\n",
    "merge_year_2023 = merge_year_dataFrame(park_2023_df, bus_2023_df, train_2023_df, house_2023_df,\n",
    "                                       develop_2023_df, demandSupply_2023_df, distribute_2023_df,\n",
    "                                       hospital_2023_df, population_2023_df, volume_2023_df,\n",
    "                                       abode_house_2023_df, priceRelative_2023_df, seoul_area_df,\n",
    "                                       year = 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ededd6-b363-496e-bee8-fee6990434f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "                              /////--     머신러닝용 데이터 결합     --/////\n",
    "'''\n",
    "concat_list = [merge_year_2015, merge_year_2016, merge_year_2017, merge_year_2018,\n",
    "               merge_year_2019, merge_year_2020, merge_year_2021, merge_year_2022,\n",
    "               merge_year_2023]\n",
    "ai_concat = pd.concat(concat_list, ignore_index = True)\n",
    "ai_concat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c18efd6-257c-4a7b-85aa-3fba47ed790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_concat = fillna_with_neighbor_mean(ai_concat, '버스 수', '연도')\n",
    "ai_concat = fillna_with_neighbor_mean(ai_concat, '역개수', '연도')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00d2973-8ef1-48b1-bad1-89c82d5849fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 데이터 타입 실수형으로 전환\n",
    "ai_concat = df_to_float(ai_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d450e6a-e5e5-4521-8c5c-73ccd37f8344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평균시세 값을 억 단위로 변환\n",
    "# 평균시세 값이 너무 크면 오차도 너무 크게 나오기 때문\n",
    "ai_concat['평균시세'] = round(ai_concat['평균시세'] / 10000, 6)\n",
    "\n",
    "# 컬럼명을 '평균시세()'으로 변경\n",
    "ai_concat.rename(columns={'평균시세': '평균시세(억)',\n",
    "                          '계_면적 (㎡)' : '개발면적(㎡)',\n",
    "                          '합계' : '합계_유통업체',\n",
    "                          '내국인-계' : '내국인',\n",
    "                          '외국인-계' : '외국인',\n",
    "                          '계' : '합계_주택',\n",
    "                          '면적 (km²)' : '행정구별 면적 (km²)'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b720c9-ae38-4878-aca2-fbd2457f16d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불필요한 변수 제거\n",
    "# ai_concat = ai_concat.drop(columns=[\"합계\", \"총인구\", '총 주택 합계'], errors=\"ignore\")\n",
    "\n",
    "# 파생변수 생성\n",
    "ai_concat[\"1인당_공원수\"] = ai_concat[\"합계_공원수\"] / ai_concat[\"총인구\"]\n",
    "# ai_concat.drop(columns = '합계_공원수', inplace=True)\n",
    "ai_concat[\"1인당_병원수\"] = ai_concat[\"병원수\"] / ai_concat[\"총인구\"]\n",
    "# ai_concat.drop(columns = '병원수', inplace=True)\n",
    "ai_concat[\"1인당_유통시설\"] = ai_concat[\"합계_유통업체\"] / ai_concat[\"총인구\"]\n",
    "# ai_concat.drop(columns = ['합계', '대형마트', '백화점', '전문점', '쇼핑센터', '복합쇼핑몰', '그밖의 대규모점포'], inplace=True)\n",
    "ai_concat['교통 접근성'] = (ai_concat['버스 수'] + ai_concat['역개수']) / ai_concat['총인구']\n",
    "ai_concat['총 인프라 수'] = (\n",
    "  ai_concat[\"버스 수\"] +\n",
    "  ai_concat[\"역개수\"] +\n",
    "  ai_concat['합계_유통업체'] +\n",
    "  ai_concat['병원수']\n",
    ")\n",
    "ai_concat['인프라 밀집도'] = ai_concat['총 인프라 수'] / ai_concat['행정구별 면적 (km²)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23157e1b-7d6a-4335-b374-a9c6ac005c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "              ///////    선택 1)   21 ~ 23 년도 데이터를 전부 섞은 후    ///////\n",
    "                   8 : 2 비율로 무작위로 나눠서 훈련용, 테스트용으로 분리하기\n",
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 독립변수와 종속변수 분리\n",
    "X = ai_concat.drop(columns=['평균시세(억)', '구', '연도'])  # 종속변수와 제외할 컬럼 제거\n",
    "y = ai_concat['평균시세(억)']  # 종속변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3cb372-6a5a-4443-9a54-7015fe69ac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 독립변수와 종속변수 간의 상관관계 계산\n",
    "# correlations = X.corrwith(y).sort_values(ascending=False)\n",
    "# print(correlations)\n",
    "# # 상관계수가 절대값 0.1 미만인 변수 제거\n",
    "# threshold = 0.1\n",
    "# low_corr_features = correlations[correlations.abs() < threshold].index\n",
    "# # low_corr_features = low_corr_features.drop(['전기대비증감률', '수요공급지수'])\n",
    "# print(\"상관관계 낮은 변수:\", low_corr_features)\n",
    "# # 상관계수 낮은 변수 제거\n",
    "# X_filtered = X.drop(columns=low_corr_features)\n",
    "# X_filtered.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16291e49-5db3-4786-84b2-275e71f4342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분리: 80% 훈련용, 20% 테스트용\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 확인\n",
    "print(\"훈련 데이터 크기:\", X_train.shape, y_train.shape)\n",
    "print(\"테스트 데이터 크기:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1006fba-32d3-40c6-a901-abf0291dd993",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####### 파이프라인을 이용해서\n",
    "### 랜덤포레스트, XGBoost, LightGBM, 그래디언트 부스팅, 릿지, 라쏘 모델\n",
    "## 학습\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 모델 정의\n",
    "models = [\n",
    "    ('RandomForest', RandomForestRegressor(n_estimators=100, random_state=42)),\n",
    "    ('XGBoost', xgb.XGBRegressor(n_estimators=100, random_state=42)),\n",
    "    ('LightGBM', lgb.LGBMRegressor(n_estimators=100, random_state=42)),\n",
    "    ('GradientBoosting', GradientBoostingRegressor(n_estimators=100, random_state=42)),\n",
    "    ('Ridge', Ridge(alpha=1.0)),\n",
    "    ('Lasso', Lasso(alpha=0.1))\n",
    "]\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "for name, model in models:\n",
    "    print(f\"######### {name} #########\")\n",
    "    \n",
    "    # 파이프라인 구성\n",
    "    pipeline = Pipeline([\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # 학습\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # 예측\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # 평가\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "    print(f\"R2 Score: {r2:.2f}\")\n",
    "    \n",
    "    # 실제 데이터, 예측 데이터, 오차 계산\n",
    "    comparison_df = pd.DataFrame({\n",
    "        '구': ai_concat.iloc[X_test.index]['구'],\n",
    "        '평균시세(억)': y_test.values,\n",
    "        '예측_평균시세': y_pred\n",
    "    })\n",
    "    comparison_df['오차'] = np.abs(comparison_df['평균시세(억)'] - comparison_df['예측_평균시세'])\n",
    "    comparison_df = comparison_df.sort_values('구')\n",
    "    display(comparison_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d80ff80-76ef-4679-af88-a5d55f02d97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### 이상치 제거\n",
    "### 적용해보기\n",
    "\n",
    "# 1. Q1, Q3 계산\n",
    "Q1 = ai_concat['평균시세(억)'].quantile(0.25)  # 1분위수\n",
    "Q3 = ai_concat['평균시세(억)'].quantile(0.75)  # 3분위수\n",
    "IQR = Q3 - Q1                           # IQR 계산\n",
    "\n",
    "# 2. 이상치 기준 설정\n",
    "lower_bound = Q1 - 1.5 * IQR            # 하한\n",
    "upper_bound = Q3 + 1.5 * IQR            # 상한\n",
    "\n",
    "# 3. 이상치 제거\n",
    "df_filtered = ai_concat[(ai_concat['평균시세(억)'] >= lower_bound) & (ai_concat['평균시세(억)'] <= upper_bound)]\n",
    "\n",
    "# 결과 출력\n",
    "print(\"원본 데이터프레임:\")\n",
    "display(ai_concat)\n",
    "print(\"\\n이상치 제거 후 데이터프레임:\")\n",
    "display(df_filtered)\n",
    "\n",
    "# Q1, Q3, IQR 및 경계 출력\n",
    "print(\"\\nQ1:\", Q1)\n",
    "print(\"Q3:\", Q3)\n",
    "print(\"IQR:\", IQR)\n",
    "print(\"Lower Bound:\", lower_bound)\n",
    "print(\"Upper Bound:\", upper_bound)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ff0bf0-1eaa-4032-bd86-c3da7dffbc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''                                     이상치 제거 후\n",
    "              ///////    선택 1)   21 ~ 23 년도 데이터를 전부 섞은 후    ///////\n",
    "                   8 : 2 비율로 무작위로 나눠서 훈련용, 테스트용으로 분리하기\n",
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 독립변수와 종속변수 분리\n",
    "X = df_filtered.drop(columns=['평균시세(억)', '구'])  # 종속변수와 제외할 컬럼 제거\n",
    "y = df_filtered['평균시세(억)']  # 종속변수\n",
    "\n",
    "# 데이터 분리: 80% 훈련용, 20% 테스트용\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 확인\n",
    "print(\"훈련 데이터 크기:\", X_train.shape, y_train.shape)\n",
    "print(\"테스트 데이터 크기:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82cbf81-4118-49b8-b977-0969e74221b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# XGBoost 모델 정의\n",
    "xgb_model = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "# 그리드 서치 하이퍼파라미터 공간 정의\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # 트리의 개수\n",
    "    'learning_rate': [0.01, 0.05, 0.1],  # 학습률\n",
    "    'max_depth': [3, 5, 7],  # 트리의 최대 깊이\n",
    "    'subsample': [0.8, 1.0],  # 데이터를 샘플링할 비율\n",
    "    'colsample_bytree': [0.8, 1.0]  # 특성 샘플링 비율\n",
    "}\n",
    "\n",
    "# GridSearchCV 정의\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, \n",
    "                           scoring='neg_mean_squared_error', cv=3, verbose=2, n_jobs=-1)\n",
    "\n",
    "# 그리드 서치 학습\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 최적의 파라미터 출력\n",
    "print(\"Best Parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# 최적 파라미터로 예측\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "y_pred_xgb = best_xgb_model.predict(X_test)\n",
    "\n",
    "# 평가\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(f\"Best XGBoost Mean Squared Error: {mse_xgb:.2f}\")\n",
    "print(f\"Best XGBoost R2 Score: {r2_xgb:.2f}\")\n",
    "\n",
    "# 실제 데이터, 예측 데이터, 오차 계산\n",
    "comparison_df_xgb = pd.DataFrame({\n",
    "    '구': ai_concat.iloc[X_test.index]['구'],\n",
    "    '평균시세(억)': y_test.values,\n",
    "    '예측_평균시세': y_pred_xgb\n",
    "})\n",
    "comparison_df_xgb['오차'] = np.abs(comparison_df_xgb['평균시세(억)'] - comparison_df_xgb['예측_평균시세'])\n",
    "comparison_df_xgb = comparison_df_xgb.sort_values('구')\n",
    "display(comparison_df_xgb)\n",
    "# 예측 결과와 실제 결과의 크기 확인\n",
    "print(f\"y_test size: {y_test.shape}\")\n",
    "print(f\"y_pred size: {y_pred.shape}\")\n",
    "\n",
    "# 예측값 크기 확인 후 제대로 일치하는지 체크\n",
    "y_pred_xgb = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# 인덱스 재조정: X_test의 인덱스를 y_test와 맞추기\n",
    "comparison_df_xgb = pd.DataFrame({\n",
    "    '구': ai_concat.iloc[X_test.index]['구'],  # X_test와 관련된 인덱스 사용\n",
    "    '평균시세(억)': y_test.values,\n",
    "    '예측_평균시세': y_pred_xgb\n",
    "})\n",
    "\n",
    "# 오차 계산\n",
    "comparison_df_xgb['오차'] = np.abs(comparison_df_xgb['평균시세(억)'] - comparison_df_xgb['예측_평균시세'])\n",
    "comparison_df_xgb = comparison_df_xgb.sort_values('구')\n",
    "\n",
    "# 확인 출력\n",
    "display(comparison_df_xgb)\n",
    "\n",
    "# 시각화 함수 호출\n",
    "plot_model_performance(comparison_df_xgb, y_test, y_pred_xgb, grid_search.best_estimator_, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2474304-2e22-45db-bee4-58c5768ecec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "best_params = grid_search.best_params_\n",
    "# 최적 파라미터로 XGBRegressor 모델 생성\n",
    "model_xgb = XGBRegressor(**best_params)\n",
    "\n",
    "# 모델을 학습\n",
    "model_xgb.fit(X_train, y_train)\n",
    "\n",
    "# 예측\n",
    "y_pred_xgb = model_xgb.predict(X_test)\n",
    "\n",
    "# 평가\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(f\"Best XGBoost Mean Squared Error: {mse_xgb:.2f}\")\n",
    "print(f\"Best XGBoost R2 Score: {r2_xgb:.2f}\")\n",
    "\n",
    "# 실제 데이터, 예측 데이터, 오차 계산\n",
    "comparison_df_xgb = pd.DataFrame({\n",
    "    '구': ai_concat.iloc[X_test.index]['구'],\n",
    "    '평균시세(억)': y_test.values,\n",
    "    '예측_평균시세': y_pred_xgb\n",
    "})\n",
    "comparison_df_xgb['오차'] = np.abs(comparison_df_xgb['평균시세(억)'] - comparison_df_xgb['예측_평균시세'])\n",
    "comparison_df_xgb = comparison_df_xgb.sort_values('구')\n",
    "display(comparison_df_xgb)\n",
    "\n",
    "# 예측 결과와 실제 결과의 크기 확인\n",
    "print(f\"y_test size: {y_test.shape}\")\n",
    "print(f\"y_pred size: {y_pred.shape}\")\n",
    "\n",
    "# 예측값 크기 확인 후 제대로 일치하는지 체크\n",
    "y_pred_xgb = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# 인덱스 재조정: X_test의 인덱스를 y_test와 맞추기\n",
    "comparison_df_xgb = pd.DataFrame({\n",
    "    '구': ai_concat.iloc[X_test.index]['구'],  # X_test와 관련된 인덱스 사용\n",
    "    '평균시세(억)': y_test.values,\n",
    "    '예측_평균시세': y_pred_xgb\n",
    "})\n",
    "\n",
    "# 오차 계산\n",
    "comparison_df_xgb['오차'] = np.abs(comparison_df_xgb['평균시세(억)'] - comparison_df_xgb['예측_평균시세'])\n",
    "comparison_df_xgb = comparison_df_xgb.sort_values('구')\n",
    "\n",
    "# 시각화 함수 호출\n",
    "plot_model_performance(comparison_df_xgb, y_test, y_pred_xgb, grid_search.best_estimator_, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd0a622-4215-49b3-8d47-16e7cb874128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3. 피처 중요도 계산\n",
    "# feature_importances = model_xgb.feature_importances_\n",
    "# importance_df = pd.DataFrame({\n",
    "#     'Feature': X.columns,\n",
    "#     'Importance': feature_importances\n",
    "# }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# # 5. 중요도가 낮은 변수 제거 (임계값 설정)\n",
    "# threshold = 0.0125  # 중요도가 0.0125보다 낮은 변수 제거\n",
    "# low_importance_features = importance_df[importance_df['Importance'] < threshold]['Feature']\n",
    "# print(f\"제거할 변수: {list(low_importance_features)}\")\n",
    "# X_reduced = X.drop(columns=low_importance_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9dd13b-1bca-4d25-bc12-47c569668a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = X.drop(columns='구성비 (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b329e0-19c4-44fb-b119-820ef70b5763",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "                                       구성비 (%) 제거 후 재학습 시켜봄\n",
    "'''\n",
    "\n",
    "# 1. 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. XGBRegressor 모델을 학습할 때 사용되는 최적의 파라미터\n",
    "best_params = grid_search.best_params_\n",
    "model_xgb = XGBRegressor(**best_params)\n",
    "\n",
    "# 3. 모델 학습\n",
    "model_xgb.fit(X_train, y_train)\n",
    "\n",
    "# 4. 예측\n",
    "y_pred_xgb = model_xgb.predict(X_test)\n",
    "\n",
    "# 5. 모델 평가\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "# 평가 지표 출력\n",
    "print(f\"Best XGBoost Mean Squared Error: {mse_xgb:.2f}\")\n",
    "print(f\"Best XGBoost R2 Score: {r2_xgb:.2f}\")\n",
    "\n",
    "# 6. 실제 데이터, 예측 데이터, 오차 계산\n",
    "comparison_df_xgb = pd.DataFrame({\n",
    "    '구': ai_concat.iloc[X_test.index]['구'],\n",
    "    '평균시세(억)': y_test.values,\n",
    "    '예측_평균시세': y_pred_xgb\n",
    "})\n",
    "\n",
    "# 오차 계산\n",
    "comparison_df_xgb['오차'] = np.abs(comparison_df_xgb['평균시세(억)'] - comparison_df_xgb['예측_평균시세'])\n",
    "comparison_df_xgb = comparison_df_xgb.sort_values('구')\n",
    "display(comparison_df_xgb)\n",
    "\n",
    "# 예측 결과와 실제 결과의 크기 확인\n",
    "print(f\"y_test size: {y_test.shape}\")\n",
    "print(f\"y_pred size: {y_pred_xgb.shape}\")\n",
    "\n",
    "# 7. 인덱스 재조정: X_test의 인덱스를 y_test와 맞추기\n",
    "comparison_df_xgb = pd.DataFrame({\n",
    "    '구': ai_concat.iloc[X_test.index]['구'],  # X_test와 관련된 인덱스 사용\n",
    "    '평균시세(억)': y_test.values,\n",
    "    '예측_평균시세': y_pred_xgb\n",
    "})\n",
    "\n",
    "# 오차 계산\n",
    "comparison_df_xgb['오차'] = np.abs(comparison_df_xgb['평균시세(억)'] - comparison_df_xgb['예측_평균시세'])\n",
    "comparison_df_xgb = comparison_df_xgb.sort_values('구')\n",
    "\n",
    "# 8. 시각화 함수 호출\n",
    "plot_model_performance(comparison_df_xgb, y_test, y_pred_xgb, model_xgb, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3426ae-522a-4bb4-aa46-d7a16cbc8a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "                                       구성비 (%) 제거 후 재학습 시켜봄\n",
    "'''\n",
    "\n",
    "# 1. 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. XGBRegressor 모델을 학습할 때 사용되는 최적의 파라미터\n",
    "best_params = grid_search.best_params_\n",
    "model_xgb = XGBRegressor(**best_params)\n",
    "\n",
    "# 3. 모델 학습\n",
    "model_xgb.fit(X_train, y_train)\n",
    "\n",
    "# 4. 예측\n",
    "y_pred_xgb = model_xgb.predict(X_test)\n",
    "\n",
    "# 5. 모델 평가\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "# 평가 지표 출력\n",
    "print(f\"Best XGBoost Mean Squared Error: {mse_xgb:.2f}\")\n",
    "print(f\"Best XGBoost R2 Score: {r2_xgb:.2f}\")\n",
    "\n",
    "# 6. 실제 데이터, 예측 데이터, 오차 계산\n",
    "comparison_df_xgb = pd.DataFrame({\n",
    "    '구': ai_concat.iloc[X_test.index]['구'],\n",
    "    '평균시세(억)': y_test.values,\n",
    "    '예측_평균시세': y_pred_xgb\n",
    "})\n",
    "\n",
    "# 오차 계산\n",
    "comparison_df_xgb['오차'] = np.abs(comparison_df_xgb['평균시세(억)'] - comparison_df_xgb['예측_평균시세'])\n",
    "comparison_df_xgb = comparison_df_xgb.sort_values('구')\n",
    "display(comparison_df_xgb)\n",
    "\n",
    "# 예측 결과와 실제 결과의 크기 확인\n",
    "print(f\"y_test size: {y_test.shape}\")\n",
    "print(f\"y_pred size: {y_pred_xgb.shape}\")\n",
    "\n",
    "# 7. 인덱스 재조정: X_test의 인덱스를 y_test와 맞추기\n",
    "comparison_df_xgb = pd.DataFrame({\n",
    "    '구': ai_concat.iloc[X_test.index]['구'],  # X_test와 관련된 인덱스 사용\n",
    "    '평균시세(억)': y_test.values,\n",
    "    '예측_평균시세': y_pred_xgb\n",
    "})\n",
    "\n",
    "# 오차 계산\n",
    "comparison_df_xgb['오차'] = np.abs(comparison_df_xgb['평균시세(억)'] - comparison_df_xgb['예측_평균시세'])\n",
    "comparison_df_xgb = comparison_df_xgb.sort_values('구')\n",
    "\n",
    "# 8. 시각화 함수 호출\n",
    "plot_model_performance(comparison_df_xgb, y_test, y_pred_xgb, model_xgb, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3818b6a5-f93e-4ab0-93db-451db972a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. 학습용 데이터를 다시 나누기 (8:2 비율로 학습용 데이터와 검증용 데이터 분리)\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. XGBRegressor 모델을 학습할 때 사용되는 최적의 파라미터\n",
    "best_params = grid_search.best_params_\n",
    "model_xgb = XGBRegressor(**best_params)\n",
    "\n",
    "# 4. 학습용 데이터로 모델 학습\n",
    "model_xgb.fit(X_train_final, y_train_final)\n",
    "\n",
    "# 5. 검증용 데이터로 예측 및 평가\n",
    "y_val_pred = model_xgb.predict(X_val)\n",
    "mse_val = mean_squared_error(y_val, y_val_pred)\n",
    "r2_val = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# 검증 데이터 평가 지표 출력\n",
    "print(f\"Validation Mean Squared Error: {mse_val:.2f}\")\n",
    "print(f\"Validation R2 Score: {r2_val:.2f}\")\n",
    "\n",
    "# 6. 최종 테스트용 데이터로 예측\n",
    "y_test_pred = model_xgb.predict(X_test)\n",
    "\n",
    "# 7. 테스트 데이터 평가\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# 테스트 데이터 평가 지표 출력\n",
    "print(f\"Test Mean Squared Error: {mse_test:.2f}\")\n",
    "print(f\"Test R2 Score: {r2_test:.2f}\")\n",
    "\n",
    "# 8. 실제 데이터, 예측 데이터, 오차 계산 (테스트 데이터 기준)\n",
    "comparison_df_xgb = pd.DataFrame({\n",
    "    '구': ai_concat.iloc[X_test.index]['구'],\n",
    "    '평균시세(억)': y_test.values,\n",
    "    '예측_평균시세': y_test_pred\n",
    "})\n",
    "\n",
    "# 오차 계산\n",
    "comparison_df_xgb['오차'] = np.abs(comparison_df_xgb['평균시세(억)'] - comparison_df_xgb['예측_평균시세'])\n",
    "comparison_df_xgb = comparison_df_xgb.sort_values('구')\n",
    "display(comparison_df_xgb)\n",
    "\n",
    "# 예측 결과와 실제 결과의 크기 확인\n",
    "print(f\"y_test size: {y_test.shape}\")\n",
    "print(f\"y_test_pred size: {y_test_pred.shape}\")\n",
    "\n",
    "# 9. 인덱스 재조정: X_test의 인덱스를 y_test와 맞추기\n",
    "comparison_df_xgb = pd.DataFrame({\n",
    "    '구': ai_concat.iloc[X_test.index]['구'],  # X_test와 관련된 인덱스 사용\n",
    "    '평균시세(억)': y_test.values,\n",
    "    '예측_평균시세': y_test_pred\n",
    "})\n",
    "\n",
    "# 오차 계산\n",
    "comparison_df_xgb['오차'] = np.abs(comparison_df_xgb['평균시세(억)'] - comparison_df_xgb['예측_평균시세'])\n",
    "comparison_df_xgb = comparison_df_xgb.sort_values('구')\n",
    "\n",
    "# 10. 시각화 함수 호출\n",
    "plot_model_performance(comparison_df_xgb, y_test, y_test_pred, model_xgb, X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a7a407-728c-4d98-be2d-f8353052c21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. 연도별 데이터를 준비 (예시: '연도'와 '구'를 기준으로)\n",
    "# ai_concat 데이터프레임에 '연도'와 '구' 열이 있어야 한다고 가정\n",
    "# 예를 들어, ai_concat에는 연도와 행정구별 데이터를 포함하고 있다고 가정\n",
    "\n",
    "# '연도' 컬럼을 기준으로 각 행정구의 데이터로 묶어줍니다.\n",
    "# X에 연도 정보를 포함하고 있어야 합니다. (예시: '연도' 컬럼)\n",
    "# 예: X['연도'] <- 연도 정보가 포함된 열\n",
    "\n",
    "# 연도별로 예측값을 계산\n",
    "yearly_predictions = []\n",
    "\n",
    "for year in X['연도'].unique():\n",
    "    # 특정 연도에 해당하는 데이터만 선택\n",
    "    X_year = X[X['연도'] == year]\n",
    "    \n",
    "    # 예측: 연도별로 각 행정구의 평균시세 예측\n",
    "    y_pred_year = model_xgb.predict(X_year)\n",
    "    \n",
    "    # 예측 결과와 '구' 정보를 결합\n",
    "    year_predictions_df = pd.DataFrame({\n",
    "        '구': X_year['구'],  # '구' 정보\n",
    "        '연도': [year] * len(X_year),\n",
    "        '예측_평균시세': y_pred_year\n",
    "    })\n",
    "    \n",
    "    # 연도별 예측값을 리스트에 추가\n",
    "    yearly_predictions.append(year_predictions_df)\n",
    "\n",
    "# 2. 모든 연도에 대한 예측값 결합\n",
    "yearly_predictions_df = pd.concat(yearly_predictions)\n",
    "\n",
    "# 3. 연도별로 각 행정구의 평균시세 추세선 그리기\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "sns.lineplot(data=yearly_predictions_df, x='연도', y='예측_평균시세', hue='구', marker='o')\n",
    "\n",
    "# 시각화 설정\n",
    "plt.title('연도별 각 행정구의 평균시세 추세선')\n",
    "plt.xlabel('연도')\n",
    "plt.ylabel('평균시세(억)')\n",
    "plt.legend(title='행정구', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# 그래프 출력\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76109fbc-d5b7-47c0-91c5-b5a747de6ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "중요도 시각화 \n",
    "'''\n",
    "xgb.plot_importance(model_xgb, importance_type='weight', max_num_features=10, title=\"Top 10 Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b8635e-d53f-4937-8393-e651f617f0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "예측값과 실제값 비교 (모델이 얼마나 잘 수행했나)\n",
    "'''\n",
    "plt.scatter(y_val, y_val_pred)\n",
    "plt.plot([min(y_val), max(y_val)], [min(y_val), max(y_val)], color='red', lw=2)\n",
    "plt.title('Predicted vs Actual')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e7838b-a844-44d3-b878-b07d90293396",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "예측값 분포 실제값 분포 비\n",
    "'''\n",
    "sns.histplot(y_val, kde=True, color='blue', label='Actual Values')\n",
    "sns.histplot(y_val_pred, kde=True, color='red', label='Predicted Values')\n",
    "plt.legend()\n",
    "plt.title('Actual vs Predicted Value Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01a06cc-b518-42f4-8a70-874220e2fc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "예측 vs 실제 가격 막대플롯 (상위 N개 예측)\n",
    "어떤 예측이 가장 틀렸는지\n",
    "'''\n",
    "top_n = 10\n",
    "comparison_df_xgb['error'] = np.abs(comparison_df_xgb['오차'])\n",
    "\n",
    "# Get top N predictions with highest error\n",
    "top_predictions = comparison_df_xgb.nlargest(top_n, 'error')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(top_predictions['구'], top_predictions['오차'], color='orange')\n",
    "plt.title(f'Top {top_n} Predictions with Largest Errors')\n",
    "plt.xlabel('구')\n",
    "plt.ylabel('Error (억)')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c3732f-610a-4c6e-851a-17c6d298a105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f51ac5-0413-4014-a08e-3847586029a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
